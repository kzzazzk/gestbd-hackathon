{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f82daac",
   "metadata": {},
   "source": [
    "![Open Alex Logo](./images/openalex-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac50be",
   "metadata": {},
   "source": [
    "## PLANTEAMIENTO Y ACOTACI√ìN DE OBJETIVOS INICIALES\n",
    "El prop√≥sito inicial pretend√≠a responder a la pregunta de qu√© instituciones y pa√≠ses son los mayores divulgadores de conocimientos en cada rama del conocimiento humano.\n",
    "\n",
    "Este objetivo era totalmente cumplible si no fuese porque openalex ya nos devolv√≠a todos los datos de forma estructurada:\n",
    "\n",
    "![Diagram OpenAlex components](./images/openalex-diagram.jpg)\n",
    "\n",
    "Es decir, en este punto del proyecto carec√≠amos de fuentes de datos no estructuradas:\n",
    "\n",
    "* Por ello, optamos por dirigir el proyecto a un enfoque diferente que parte del inter√©s de comprender que tecnolog√≠as de programaci√≥n son las mas usadas, en art√≠culos o proyectos de investigaci√≥n relacionados con la computaci√≥n, y como var√≠a su uso y distribuci√≥n en funci√≥n de cada subcampo.\n",
    "\n",
    "Adem√°s, identificamos una problem√°tica con el alcance definido para el proyecto (dado que se desarrollar√≠a exclusivamente en 5h durante el Hackathon):\n",
    "\n",
    "* OpenAlex cuenta con un total de **200 Millones de archivos**, lo cual implicar√≠a una cantidad alarmante de tiempo para extraer y procesar toda la informaci√≥n necesaria para cumplir el objetivo. Por lo tanto, decidimos utilizar un conjunto de restricciones para acotar el alcance de los datos:\n",
    "\n",
    "    * Usamos el campo _Tem√°tica_ para filtrar el n√∫mero de archivos a extraer, en este aspecto decidimos filtrar una sucesi√≥n de tem√°ticas padre-hijo relacionadas con nuestra meta: \n",
    "\n",
    "        ***Physical Sciences (domain) -> Computer Science (field) -> Artificial Inteligence (subfield) -> Todas las tem√°ticas hijas de AI sin hijo (topics)***\n",
    "\n",
    "    * Agregamos un conjunto de campos restrictivos como que el idioma en el que est√©n escritos sea en ingl√©s, agregamos filtrado de keyword por lenguajes de programaci√≥n en art√≠culos y pedimos exclusivamente los articulos que tengan vinculado su pdf y adem√°s que est√© sea open access. Esto redujo el n√∫mero de obras a procesar a ~6000 art√≠culos:\n",
    "\n",
    "![Filtro tem√°ticas dentro de OpenAlex](./images/filtro_openalex.jpeg)\n",
    "\n",
    "\n",
    "Por √∫ltimo decidimos adem√°s simplificar mucho m√°s las entidades a almacenar, reduciendo lo m√°ximo posible la estructura y manteniendo la funcionalidad que se quer√≠a obtener con el objetivo. A continuaci√≥n se muestra la imagen de la estructura completa de la base de datos antes y despu√©s de modificarla:\n",
    "\n",
    "### Diagrama Entidad-Relaci√≥n del sistema antes de la adaptaci√≥n\n",
    "![Diagrama Entidad-Relaci√≥n del sistema antes](./images/relaciones%20db.png)\n",
    "### Diagrama Entidad-Relaci√≥n del sistema despu√©s de la adaptaci√≥n\n",
    "![Diagrama Entidad-Relaci√≥n del sistema despu√©s](./images/esquema_redux.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd83b5f",
   "metadata": {},
   "source": [
    "## TRABAJO REALIZADO EN EL HACKATHON\n",
    "\n",
    "### CREACI√ìN BASE DE DATOS POSTGRESQL\n",
    "---\n",
    "\n",
    "\n",
    "En nuestra base de datos de PostgreSQL representamos las entidades como las siguientes tablas:\n",
    "\n",
    "* **Obra:** Engloba los art√≠culos citados dentro de OpenAlex. Tiene dos funciones esenciales:\n",
    "    * Almacenar los datos estructurados de cada art√≠culo organizados por sus atributos: nombre, doi, url, etc.\n",
    "    * Enlazar  _Tecnologia_ y _Tematica_ para poder realizar b√∫squedas de datos filtrando por ambas entidades. Cabe destacar que, como _Tecnologia_ y _Obra_ tienen una relaci√≥n N:M, fue necesario crear una tabla para representar esta relaci√≥n. Esto no pasa con la relaci√≥n entre _Obra_ y _Tematica_ ya que, como es 1:N, con indicar un atributo *tematica_id* es suficiente.\n",
    "\n",
    "* **Tecnologia:** Engloba los distintos lenguajes de programaci√≥n obtenidos al extraer y analizar el contenido de los art√≠culos. Los datos son solamente no estructurados. Est√° enlazado con _Obra_ por *Obra_tecnologia*.\n",
    "\n",
    "* **Tematica:** Aqu√≠ se alamacena cada tem√°tica encontrada en la b√∫squeda de OpenAlex, por lo que los datos de esta tabla son estructurados. Se encuentra relacionado con _Obra_ gracias a que las obras de una tem√°tica concreta almacenan su id. Lo importante aqu√≠ es que existen tem√°ticas que son subtem√°ticas de otras. La idea es que las obras est√©n relacionadas solamente con tem√°ticas sin hijos para que estas a su vez se relacionen con sus padres, haciendo que todas las obras de cada hijo se relacionen con el padre. Para hacer esta relaci√≥n de _Tematica_ con _Tematica_, fue necesaria la creaci√≥n de una nueva tabla: *tematica_contenida*.\n",
    "\n",
    "Adem√°s, se han creado estas tablas para las relaciones:\n",
    "\n",
    "* **Obra_tecnologia:** Hace referencia a la relaci√≥n N:M entre _Tecnologia_ y _Obra_.\n",
    "\n",
    "* **Tematica_contenida:** Hace referencia a la relaci√≥n padre a hijo de _Tematica_ con _Tematica_.\n",
    "\n",
    "Todas estas tablas fueron creadas dentro de un *SQL script*. Para subirla a PostgreSQL, se conecta el docker, se indican los par√°metros necesarios en el Python y, si no hay ning√∫n problema, se suben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595cfe57",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"localhost\" (::1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Tables created or verified successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# Connect to default database to check/create demoDB\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     connection = \u001b[43mpsycopg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDB_PARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDB_PARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mport\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdemoDB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDB_PARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDB_PARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpassword\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     connection.autocommit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     90\u001b[39m     cursor = connection.cursor()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MAADM/1C/2¬™ 5S/GESTBD/gestbd-hackathon/.venv/lib/python3.12/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "DB_PARAMS = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"demoDB\",\n",
    "    \"user\": \"userPSQL\",\n",
    "    \"password\": \"passPSQL\"\n",
    "}\n",
    "\n",
    "sql_script = \"\"\"CREATE TABLE IF NOT EXISTS tematica (\n",
    "    id INTEGER,\n",
    "    nombre_campo TEXT NOT NULL,\n",
    "        PRIMARY KEY (id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS tecnologia (\n",
    "    id INTEGER,\n",
    "    nombre TEXT NOT NULL,\n",
    "    tipo TEXT,\n",
    "    version TEXT,\n",
    "    PRIMARY KEY (id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS obra (\n",
    "    id INTEGER,\n",
    "    doi TEXT UNIQUE,  -- NEW\n",
    "    direccion_fuente TEXT NOT NULL,\n",
    "    titulo TEXT NOT NULL,\n",
    "    abstract TEXT,\n",
    "    fecha_publicacion TEXT,\n",
    "    idioma TEXT,\n",
    "    num_citas INTEGER DEFAULT 0,\n",
    "    fwci REAL,\n",
    "    tematica_id INTEGER,\n",
    "    PRIMARY KEY (id),\n",
    "    FOREIGN KEY (tematica_id)\n",
    "        REFERENCES tematica(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE SET NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS obra_tecnologia (\n",
    "    id INTEGER,\n",
    "    obra_id INTEGER NOT NULL,\n",
    "    tecnologia_id INTEGER NOT NULL,\n",
    "    PRIMARY KEY (id),\n",
    "    FOREIGN KEY (obra_id)\n",
    "        REFERENCES obra(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE CASCADE,\n",
    "    FOREIGN KEY (tecnologia_id)\n",
    "        REFERENCES tecnologia(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE RESTRICT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS tematica_contenida (\n",
    "    id INTEGER,\n",
    "    tematica_padre_id INTEGER NOT NULL,\n",
    "    tematica_hijo_id INTEGER NOT NULL,\n",
    "    PRIMARY KEY (id),\n",
    "    FOREIGN KEY (tematica_padre_id)\n",
    "        REFERENCES tematica(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE CASCADE,\n",
    "    FOREIGN KEY (tematica_hijo_id)\n",
    "        REFERENCES tematica(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE CASCADE,\n",
    "    CHECK (tematica_padre_id <> tematica_hijo_id)\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_obra_tematica ON obra(tematica_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_obratec_tecnologia ON obra_tecnologia(tecnologia_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_tematica_hijo ON tematica_contenida(tematica_hijo_id);\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    # Connect to default database to check/create demoDB\n",
    "    connection = psycopg2.connect(\n",
    "        host=DB_PARAMS['host'],\n",
    "        port=DB_PARAMS['port'],\n",
    "        database=\"demoDB\",\n",
    "        user=DB_PARAMS['user'],\n",
    "        password=DB_PARAMS['password']\n",
    "    )\n",
    "    connection.autocommit = True\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT 1 FROM pg_database WHERE datname = %s;\", (DB_PARAMS[\"database\"],))\n",
    "    exists = cursor.fetchone()\n",
    "\n",
    "    if not exists:\n",
    "        cursor.execute(sql.SQL(f\"CREATE DATABASE {DB_PARAMS['database']};\"))\n",
    "        print(f\"‚úÖ Database '{DB_PARAMS['database']}' created.\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Database '{DB_PARAMS['database']}' already exists.\")\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    # Connect to demoDB to create tables\n",
    "    connection = psycopg2.connect(**DB_PARAMS)\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(sql_script)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"‚úÖ Tables created or verified successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962917fe",
   "metadata": {},
   "source": [
    "### OBTENCI√ìN DATOS ESTRUCTURADOS (REQUESTS & CSV)\n",
    "---\n",
    "\n",
    "La idea ahora es obtener los datos estructurados, es decir, el contenido de _Obra_, _Tematica_ y *Tematica_contenida*. Para utilizar posteriormente estos datos, se ha creado una carpeta **cache** para almacenar estas tablas ya con la informaci√≥n estructurada en formato csv. Para explicar el proceso completo de esta fase, podemos dividirla en las siguientes subfases:\n",
    "\n",
    "1. Creaci√≥n del csv de la entidad _obra_\n",
    "2. B√∫squeda y obtenci√≥n de los art√≠culos pedidos.\n",
    "3. Creaci√≥n del csv de la entidad _tematica_\n",
    "4. Creaci√≥n del csv de la relaci√≥n *tematica_contenida*\n",
    "\n",
    "En el c√≥digo, se puede observar que el main sigue una estructura equivalente:\n",
    "\n",
    "```\n",
    "def main():\n",
    "    initialize_csv_files()\n",
    "    tematica_map = fetch_all_works()\n",
    "    save_tematica_csv(tematica_map)\n",
    "    update_tematica_and_generate_tematica_contenida()\n",
    "```\n",
    "\n",
    "Adem√°s, cabe destacar que se han declarado variables al inicio del c√≥digo para que la b√∫squeda siga los filtros explicados en la introducci√≥n y para tener ya almacenadas las direcciones donde se crearan los csvs:\n",
    "\n",
    "```\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "PER_PAGE = 200\n",
    "KEYWORDS = [\n",
    "    \"python\",\"c-programming-language\",\"javascript\",\"java\",\"java-programming-language\",\n",
    "    \"sql\",\"dart\",\"swift\",\"cobol\",\"fortran\",\"matlab\",\"prolog\",\"lisp\",\"haskell\",\"rust\",\"perl\",\n",
    "    \"scala\",\"html\",\"html5\"\n",
    "]\n",
    "SUBFIELD_ID = \"subfields/1702\"\n",
    "LANGUAGE = \"languages/en\"\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # go up one level from src/\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
    "\n",
    "CSV_OBRA = os.path.join(CACHE_DIR, \"obra.csv\")\n",
    "CSV_TEMATICA = os.path.join(CACHE_DIR, \"tematica.csv\")\n",
    "CSV_TEMATICA_CONTENIDA = os.path.join(CACHE_DIR, \"tematica_contenida.csv\")\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "BASE_TOPICS = [\"Physical Sciences\", \"Computer Science\", \"Artificial Intelligence\"]\n",
    "```\n",
    "#### Creaci√≥n del csv de la entidad _obra_\n",
    "Crea el archivo *obra.csv* (o lo abre si ya esta creado) y crea la tabla vac√≠a (atributos son las columnas y habr√° tantas filas como obras distintas). Este proceso se encuentra represnetado dentro de la funci√≥n *initialize_csv_files*.\n",
    "\n",
    "#### B√∫squeda y obtenci√≥n de los art√≠culos pedidos.\n",
    "Como no todas las obras aparecen por p√°gina (l√≠mite: **200**). Este proceso se ha creado siguiendo esta l√≥gica:\n",
    "1. La b√∫squeda se va a realizar en bucle, por lo que es necesario crear los valores no afectados antes de empezar. Las que tienen mayor importancia son la listado/diccionario de tem√°ticas (*tematic_map*), el id de la tem√°tica siguiente (*next_tematica_id*) y la p√°gina actual (*page*).\n",
    "\n",
    "2. Empieza al bucle. Al inicio se realiza la llamada a OpenAlex. A esta se le pasa por par√°metros la p√°gina actual (*page*), el l√≠mite de 200 (*PER_PAGE*), un filtro formado por los datos declarados previamente (*filter*) y el orden (*sort*). Solo en caso deque la funci√≥n que realiza la llamada (*fetch_page*) devuelva los resultados se sigue con el proceso.\n",
    "\n",
    "3. Saca todos los datos estrcuturados para las tres tablas, usando el identificador correspondiente de cada atributo dentro de OpenAlex. El abstract se restructura para que se pueda insertar en el csv (*reconstruct_abstract*)\n",
    "\n",
    "4. En cuanto a la tem√°tica. Se obtiene su nombre. Si ya est√° en *tematic_map* se ignora. Si no, se a√±ade al diccionario, siendo el nombre la clave y su valor el *next_tematica_id*, el cual es su nuevo id. Al valor de esta √∫ltima variable se le suma uno para la siguiente ejecuci√≥n.\n",
    "\n",
    "5. Se saca *tematica_id* con *tematic_map* y se crea una nueva fila en *obra_csv* con los datos del art√≠culo. Si el bucle ha terminado con los art√≠culos de la p√°gina, va a la siguiente (*page* + 1) y repite el bucle. Si ya no se encuentran m√°s obras, se finaliza.\n",
    "\n",
    "6. Se devuelve *tematic_map* para la creaci√≥n de los otros dos csv.\n",
    "\n",
    "La funci√≥n que sigue este proceso es *fetch_all_works*.\n",
    "\n",
    "#### Creaci√≥n del csv de la entidad _tematica_\n",
    "Se crea el *tematica_csv* tal cual se cre√≥ el de _obra_, pero como se tienen ya el n√∫mero de tem√°ticas con sus ids gracias a *tematic_map*, se llena la tabla al completo con todas las tem√°ticas. La funci√≥n que sigue este proceso es *save_tematica_csv*.\n",
    "\n",
    "#### Creaci√≥n del csv de la relaci√≥n *tematica_contenida*\n",
    "Esta funci√≥n se ha creado con la idea de que ya sabemos quienes son los padres: ***Physical Sciences -> Computer_Science -> Artificial Inteligence -> Tem√°ticas sin hijo***. Por tanto, a partir de cada tem√°tica dentro de *tematica_csv*, se analiza si es una de los posibles padres. Si es una sin hijos, se declara como hija de *Artificial Inteligence* con un identificador propio (m√°s el de *Artificial Inteligence* y el hijo). Para las que se son padres, se decalra directamente la relaci√≥n a partir de la estructura antes mencionada. Con esto, se crea *tematica_contenida.csv*  y en este archivo se primero los nombres de las columnas (*id*, *id_padre* e *id_hijo*) y posteriormente tantas filas como relaciones detectadas (ids distintos), incluidas las relaciones entre los padres. La funci√≥n que sigue este proceso es *update_tematica_and_generate_tematica_contenida*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e722b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "PER_PAGE = 200\n",
    "KEYWORDS = [\n",
    "    \"python\",\"c-programming-language\",\"javascript\",\"java\",\"java-programming-language\",\n",
    "    \"sql\",\"dart\",\"swift\",\"cobol\",\"fortran\",\"matlab\",\"prolog\",\"lisp\",\"haskell\",\"rust\",\"perl\",\n",
    "    \"scala\",\"html\",\"html5\"\n",
    "]\n",
    "SUBFIELD_ID = \"subfields/1702\"\n",
    "LANGUAGE = \"languages/en\"\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # go up one level from src/\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
    "\n",
    "CSV_OBRA = os.path.join(CACHE_DIR, \"obra.csv\")\n",
    "CSV_TEMATICA = os.path.join(CACHE_DIR, \"tematica.csv\")\n",
    "CSV_TEMATICA_CONTENIDA = os.path.join(CACHE_DIR, \"tematica_contenida.csv\")\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "BASE_TOPICS = [\"Physical Sciences\", \"Computer Science\", \"Artificial Intelligence\"]\n",
    "\n",
    "def reconstruct_abstract(abstract_inverted_index):\n",
    "    if not abstract_inverted_index or not isinstance(abstract_inverted_index, dict):\n",
    "        return \"\"\n",
    "    position_map = {}\n",
    "    for word, positions in abstract_inverted_index.items():\n",
    "        for pos in positions:\n",
    "            position_map[pos] = word\n",
    "    return \" \".join(position_map[pos] for pos in sorted(position_map.keys()))\n",
    "\n",
    "def fetch_page(url, params, max_retries=5, delay_base=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            print(f\"‚ö†Ô∏è Warning: Bad response {response.status_code}, retrying...\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Exception during request: {e}\")\n",
    "        retries += 1\n",
    "        time.sleep(delay_base ** retries)\n",
    "    print(f\"‚ùå Error: Failed to fetch page after {max_retries} retries.\")\n",
    "    return None\n",
    "\n",
    "def initialize_csv_files():\n",
    "    os.makedirs(os.path.dirname(CSV_OBRA), exist_ok=True)\n",
    "    with open(CSV_OBRA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"id\",\"direccion_fuente\",\"titulo\",\"abstract\",\"fecha_publicacion\",\n",
    "            \"idioma\",\"num_citas\",\"fwci\",\"tematica_id\",\"doi\"\n",
    "        ])\n",
    "    print(f\"Initialized '{CSV_OBRA}' for writing works.\")\n",
    "\n",
    "def fetch_all_works():\n",
    "    tematica_map = {}\n",
    "    next_tematica_id = 1\n",
    "    obra_id = 1\n",
    "    page = 1\n",
    "    total_results = None\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"per_page\": PER_PAGE,\n",
    "            \"filter\": f\"open_access.is_oa:true,has_content.pdf:true,primary_topic.subfield.id:{SUBFIELD_ID},best_oa_location.is_accepted:true,language:{LANGUAGE},keywords.id:{'|'.join(KEYWORDS)}\",\n",
    "            \"sort\": \"cited_by_count:desc\"\n",
    "        }\n",
    "        data = fetch_page(BASE_URL, params)\n",
    "        if not data or \"results\" not in data:\n",
    "            print(f\"No data returned for page {page}, stopping.\")\n",
    "            break\n",
    "\n",
    "        works = data[\"results\"]\n",
    "        if total_results is None:\n",
    "            total_results = data.get(\"meta\", {}).get(\"count\", 0)\n",
    "            print(f\"Total results to fetch (approximate): {total_results}\")\n",
    "\n",
    "        print(f\"Fetched page {page} with {len(works)} works.\")\n",
    "\n",
    "        with open(CSV_OBRA, \"a\", newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for work in works:\n",
    "                # --- PDF URL ---\n",
    "                pdf_url = work.get(\"best_oa_location\", {}).get(\"pdf_url\")\n",
    "                if not pdf_url:\n",
    "                    continue\n",
    "\n",
    "                # --- DOI ---\n",
    "                doi = work.get(\"doi\", \"\")\n",
    "\n",
    "                titulo = work.get(\"title\", \"\")\n",
    "                abstract = reconstruct_abstract(work.get(\"abstract_inverted_index\"))\n",
    "                fecha_publicacion = work.get(\"publication_date\", \"\")\n",
    "                idioma = work.get(\"language\", LANGUAGE)\n",
    "                num_citas = work.get(\"cited_by_count\", 0)\n",
    "                fwci = work.get(\"fwci\", \"\")\n",
    "                primary_topic = work.get(\"primary_topic\")\n",
    "                if not primary_topic:\n",
    "                    continue\n",
    "                topic_name = primary_topic.get(\"display_name\", \"Unknown Topic\")\n",
    "                if topic_name not in tematica_map:\n",
    "                    tematica_map[topic_name] = next_tematica_id\n",
    "                    next_tematica_id += 1\n",
    "                tematica_id = tematica_map[topic_name]\n",
    "\n",
    "                writer.writerow([\n",
    "                    obra_id, pdf_url, titulo, abstract, fecha_publicacion,\n",
    "                    idioma, num_citas, fwci, tematica_id, doi\n",
    "                ])\n",
    "                obra_id += 1\n",
    "\n",
    "        if page * PER_PAGE >= total_results or not works:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    print(f\"Finished fetching all works. Total works saved: {obra_id-1}\")\n",
    "    return tematica_map\n",
    "\n",
    "def save_tematica_csv(tematica_map):\n",
    "    os.makedirs(os.path.dirname(CSV_TEMATICA), exist_ok=True)\n",
    "    with open(CSV_TEMATICA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"nombre_campo\"])\n",
    "        for topic_name, topic_id in tematica_map.items():\n",
    "            writer.writerow([topic_id, topic_name])\n",
    "    print(f\"Saved '{CSV_TEMATICA}' with {len(tematica_map)} topics.\")\n",
    "\n",
    "def update_tematica_and_generate_contenida():\n",
    "    if not os.path.exists(CSV_TEMATICA):\n",
    "        raise FileNotFoundError(f\"{CSV_TEMATICA} not found.\")\n",
    "\n",
    "    tematicas = {}\n",
    "    rows = []\n",
    "    with open(CSV_TEMATICA, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            row[\"id\"] = int(row[\"id\"])\n",
    "            rows.append(row)\n",
    "            tematicas[row[\"nombre_campo\"].strip()] = row[\"id\"]\n",
    "\n",
    "    max_id = max(r[\"id\"] for r in rows)\n",
    "    for topic in BASE_TOPICS:\n",
    "        if topic not in tematicas:\n",
    "            max_id += 1\n",
    "            tematicas[topic] = max_id\n",
    "            rows.append({\"id\": max_id, \"nombre_campo\": topic})\n",
    "            print(f\"Added base topic '{topic}' with id={max_id}\")\n",
    "\n",
    "    with open(CSV_TEMATICA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"id\", \"nombre_campo\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    relaciones = [\n",
    "        {\"id_padre\": tematicas[\"Physical Sciences\"], \"id_hijo\": tematicas[\"Computer Science\"]},\n",
    "        {\"id_padre\": tematicas[\"Computer Science\"], \"id_hijo\": tematicas[\"Artificial Intelligence\"]}\n",
    "    ]\n",
    "    ai_id = tematicas[\"Artificial Intelligence\"]\n",
    "    for nombre, id_ in tematicas.items():\n",
    "        if nombre not in BASE_TOPICS:\n",
    "            relaciones.append({\"id_padre\": ai_id, \"id_hijo\": id_})\n",
    "    relaciones = [{\"id_padre\": p, \"id_hijo\": h} for p, h in {(r[\"id_padre\"], r[\"id_hijo\"]) for r in relaciones}]\n",
    "\n",
    "    os.makedirs(os.path.dirname(CSV_TEMATICA_CONTENIDA), exist_ok=True)\n",
    "    with open(CSV_TEMATICA_CONTENIDA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"id_padre\", \"id_hijo\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(relaciones)\n",
    "\n",
    "    print(f\"'{CSV_TEMATICA}' updated with {len(rows)} topics.\")\n",
    "    print(f\"'{CSV_TEMATICA_CONTENIDA}' generated with {len(relaciones)} relations.\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting OpenAlex fetch process...\")\n",
    "    initialize_csv_files()\n",
    "    tematica_map = fetch_all_works()\n",
    "    save_tematica_csv(tematica_map)\n",
    "    update_tematica_and_generate_contenida()\n",
    "    print(\"Finished fetching and processing all works and topics.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd7bce",
   "metadata": {},
   "source": [
    "### OBTENCI√ìN DATOS NO ESTRUCTURADOS (LLM)\n",
    "Una vez establecidos los datos estructurados, comenzamos la tarea de extracci√≥n de lenguajes de programaci√≥n usando un LLM. Para la comprensi√≥n de esta fase, la dividir√© en subfases:\n",
    "\n",
    "1.  **Extracci√≥n del texto** de los documentos PDF de los art√≠culos.\n",
    "2.  **Generaci√≥n de lenguajes** de programaci√≥n por art√≠culo, enviando el texto extra√≠do de cada obra a `gpt-5-nano`.\n",
    "3.  **Guardado en cach√©** de este procesamiento en los archivos `tecnologia.csv` y `obra_tecnologia.csv` para no tener que volver a ejecutar el LLM en caso de p√©rdida de datos.\n",
    "\n",
    "\n",
    "#### EXTRACCI√ìN DEL TEXTO DE LOS DOCUMENTOS PDF DE LOS ART√çCULOS\n",
    "Esta tarea parece simple, pero realmente es la m√°s compleja para no perder datos. Veamos c√≥mo est√° estructurado el `obra.csv` para comprenderlo:\n",
    "\n",
    "![Filtro tem√°ticas dentro de OpenAlex](./images/obra-csv-screenshot.png)\n",
    "\n",
    "Como se puede observar, hay dos campos que hacen referencia al URL:\n",
    "* **`doi`**: El Digital Object Identifier, este es el identificador √∫nico del art√≠culo.\n",
    "* **`direccion_fuente`**: La *open access url* m√°s fiable desde el punto de vista de OpenAlex.\n",
    "\n",
    "La `direccion_fuente` va a ser la primera URL a la que consultaremos, ya que asumimos que la consulta a OpenAlex siempre devuelve un archivo PDF de *open access* (dado que la restricci√≥n en la consulta as√≠ lo define).\n",
    "\n",
    "El flujo de extracci√≥n es el siguiente:\n",
    "\n",
    "1.  **Intento con `direccion_fuente`**:\n",
    "    * Si la URL responde con un `Content-Type` de `application/pdf`, extraemos el texto directamente usando la librer√≠a `PyMuPDF` y pasamos al siguiente paso.\n",
    "    * Si la URL responde con `text/html`, significa que es una p√°gina web.\n",
    "\n",
    "2.  **Proceso de *Scraping* (si es HTML)**:\n",
    "    * Se analiza el HTML buscando clases comunes que los visores de PDF web usan para renderizar texto (ej: `[class*=\"textLayer\"]`). Si se encuentra texto, se extrae.\n",
    "    * Si no se encuentra texto, se buscan todos los enlaces `<a>` en la p√°gina que terminen en `.pdf` y se intenta descargar el primer enlace encontrado.\n",
    "\n",
    "3.  **Proceso de *Fallback* (si todo lo anterior falla)**:\n",
    "    * Si la `direccion_fuente` falla o no se encuentra un PDF, se utiliza el `doi` del art√≠culo.\n",
    "    * Se consulta la API de **Unpaywall** (`https://api.unpaywall.org/v2/{doi}`) para encontrar la mejor URL de PDF de acceso abierto (`best_oa_location`).\n",
    "    * Paralelamente, se intenta construir una URL directa al portal de **ACM** (`https://dl.acm.org/doi/pdf/{doi}`), que es una fuente com√∫n.\n",
    "\n",
    "4.  **Extracci√≥n Final**:\n",
    "    * Si cualquiera de estas estrategias devuelve un archivo PDF v√°lido, se extrae su contenido con `PyMuPDF`. Si ninguna lo logra, se marca el art√≠culo como fallido y se contin√∫a con el siguiente.\n",
    "\n",
    "#### MANEJO DE ERRORES (HTTP Y \"SOFT 404\")\n",
    "El flujo anterior est√° protegido contra errores de varias maneras para asegurar su robustez:\n",
    "\n",
    "* **Errores HTTP (Timeouts y Conexi√≥n):** Toda la l√≥gica de descarga (`requests.get`) est√° envuelta en un bloque `try...except` gen√©rico. Esto captura errores de red, DNS, o si el servidor tarda demasiado en responder (definido en `PDF_TIMEOUT = 30`). Si ocurre una de estas excepciones, se registra el error y se activa inmediatamente el proceso de *fallback* (Unpaywall/ACM).\n",
    "\n",
    "* **Errores \"Soft 404\" en HTML:** Este es un caso cr√≠tico. A veces, un servidor devuelve un c√≥digo `200 OK` (√©xito) pero la p√°gina HTML es en realidad un error (\"No encontrado\", \"Acceso denegado\", etc.).\n",
    "    * Para detectar esto, si la respuesta es `text/html`, el script extrae el texto plano de la p√°gina (`soup.get_text()`) y lo busca contra una lista de indicadores de error (ej: `[\"not found\", \"error 404\", \"no encontrado\", \"access denied\"]`).\n",
    "    * Si encuentra alguna de estas frases, considera la p√°gina como un error, la descarta, y activa el *fallback* a Unpaywall y ACM.\n",
    "\n",
    "* **Detecci√≥n de Contenido Bloqueado (Pre-LLM):** Existe un √∫ltimo filtro. Incluso si se extrae texto con √©xito, este podr√≠a ser in√∫til (ej. un *paywall* o un aviso de \"habilitar cookies\"). La funci√≥n `analyze_text_with_gpt` revisa los primeros 300 caracteres del texto. Si detecta frases como `[\"enable javascript and cookies to continue\", \"access denied\", ...]`, omite el an√°lisis del LLM para ese art√≠culo, evitando procesar texto basura y gastar recursos.\n",
    "\n",
    "\n",
    "#### GENERACI√ìN DE LENGUAJES V√çA LLM\n",
    "Una vez obtenido el texto crudo del art√≠culo, se env√≠a al LLM (`gpt-5-nano`) para su an√°lisis.\n",
    "\n",
    "Se utiliza un *prompt* espec√≠fico que act√∫a como un asistente de an√°lisis de texto. Este *prompt* tiene reglas claras:\n",
    "\n",
    "```\n",
    "instructions = \"\"\"\n",
    "    You are a text analysis assistant specialized in identifying programming languages mentioned in academic or technical articles.\n",
    "    Analyze the provided raw text (extracted directly from a PDF). Identify and return the main programming languages mentioned in the article (do not include frameworks, libraries, or tools).\n",
    "    If a ‚ÄúReferences‚Äù or ‚ÄúBibliography‚Äù section appears, ignore all text after that marker.\n",
    "    Return strictly in JSON, like:\n",
    "    {\n",
    "        \"programming_languages\": [\"Python\", \"C\", \"Java\"]\n",
    "    }\n",
    "    If none found:\n",
    "    {\n",
    "        \"programming_languages\": []\n",
    "    }\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "#### ALMACENAMIENTO EN CACHE (CSV)\n",
    "El proceso completo est√° orquestado por la funci√≥n `process_all_obras`, que itera sobre cada art√≠culo del `obra.csv` y aplica los pasos anteriores. Los resultados se guardan para asegurar la persistencia de los datos:\n",
    "\n",
    "* **`tecnologia.csv`**: Este archivo act√∫a como una tabla maestra para las tecnolog√≠as. Cuando el LLM devuelve un lenguaje (ej: \"Python\"), el script comprueba (con `load_tecnologias`) si \"Python\" ya existe en este CSV. Si no existe, se le asigna un nuevo `id` (`next_tecn_id`) y se a√±ade al final.\n",
    "* **`obra_tecnologia.csv`**: Este archivo es la tabla de uni√≥n (relaci√≥n N-a-N). Por cada lenguaje identificado en un art√≠culo, se guarda una fila que conecta el `obra_id` con el `tecnologia_id` correspondiente (ej: `[id_enlace, 101, 1]` para vincular la obra 101 con Python (id 1)).\n",
    "\n",
    "Este sistema de cach√© es crucial, ya que permite reanudar el proceso en caso de error sin tener que volver a consumir recursos de la API del LLM sobre art√≠culos ya procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d558a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import json\n",
    "import openai\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz \n",
    "\n",
    "\n",
    "MODEL_NAME = \"mistral:instruct\"\n",
    "PDF_TIMEOUT = 30\n",
    "UNPAYWALL_EMAIL = \"your_email@example.com\"\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
    "OBRAS_CSV = os.path.join(CACHE_DIR, \"obra.csv\")\n",
    "TECN_CSV = os.path.join(CACHE_DIR, \"tecnologia.csv\")\n",
    "OBRA_TECN_CSV = os.path.join(CACHE_DIR, \"obra_tecnologia.csv\")\n",
    "\n",
    "\n",
    "instructions = \"\"\"You are a text analysis assistant specialized in identifying programming languages mentioned in academic or technical articles.\n",
    "Analyze the provided raw text (extracted directly from a PDF). Identify and return the main programming languages mentioned in the article (do not include frameworks, libraries, or tools).\n",
    "If a ‚ÄúReferences‚Äù or ‚ÄúBibliography‚Äù section appears, ignore all text after that marker.\n",
    "Return strictly in JSON, like:\n",
    "{\n",
    "  \"programming_languages\": [\"Python\", \"C\", \"Java\"]\n",
    "}\n",
    "If none found:\n",
    "{\n",
    "  \"programming_languages\": []\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------\n",
    "# CSV helpers\n",
    "# ----------------------\n",
    "def read_obras_from_csv(file_path):\n",
    "    obras = []\n",
    "    with open(file_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            obras.append((int(row[\"id\"]), row[\"direccion_fuente\"], row.get(\"doi\")))\n",
    "    return obras\n",
    "\n",
    "def init_csv(file_path, headers=None):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
    "        with open(file_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            if headers:\n",
    "                writer.writerow(headers)\n",
    "        return 1\n",
    "    max_id = 0\n",
    "    with open(file_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                val = row.get(\"id\")\n",
    "                if val:\n",
    "                    try:\n",
    "                        max_id = max(max_id, int(val))\n",
    "                    except:\n",
    "                        continue\n",
    "        except:\n",
    "            f.seek(0)\n",
    "            for line in f:\n",
    "                parts = line.split(\",\")\n",
    "                if parts:\n",
    "                    try:\n",
    "                        max_id = max(max_id, int(parts[0].strip()))\n",
    "                    except:\n",
    "                        continue\n",
    "    return max_id + 1\n",
    "\n",
    "def append_unique_to_csv(file_path, row, headers=None, key_index=1):\n",
    "    init_csv(file_path, headers=headers)\n",
    "    existing_keys = set()\n",
    "    with open(file_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        peek = next(reader, None)\n",
    "        if headers and peek and all(h in peek for h in headers):\n",
    "            pass\n",
    "        else:\n",
    "            if peek:\n",
    "                try:\n",
    "                    existing_keys.add(peek[key_index])\n",
    "                except:\n",
    "                    pass\n",
    "        for r in reader:\n",
    "            try:\n",
    "                existing_keys.add(r[key_index])\n",
    "            except:\n",
    "                continue\n",
    "    key = row[key_index] if len(row) > key_index else None\n",
    "    if key not in existing_keys:\n",
    "        with open(file_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "def append_to_csv(file_path, row, headers=None):\n",
    "    \"\"\"Simple append without uniqueness (for obra_tecnologia)\"\"\"\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    with open(file_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists and headers:\n",
    "            writer.writerow(headers)\n",
    "        writer.writerow(row)\n",
    "def load_tecnologias(file_path):\n",
    "    tech_map = {}\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                tech_map[row[\"nombre\"]] = int(row[\"id\"])\n",
    "    return tech_map\n",
    "\n",
    "# ----------------------\n",
    "# PDF + Analysis\n",
    "# ----------------------\n",
    "def get_text_from_pdf_url(pdf_url, doi=None):\n",
    "    tried_urls = set()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    def fetch_unpaywall_pdf(doi):\n",
    "        if not doi:\n",
    "            return None\n",
    "        try:\n",
    "            unpaywall_url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "            r = requests.get(unpaywall_url, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                pdf_link = data.get(\"best_oa_location\", {}).get(\"url_for_pdf\")\n",
    "                if pdf_link:\n",
    "                    print(f\"üìñ Found Unpaywall PDF: {pdf_link}\")\n",
    "                    return pdf_link\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Unpaywall fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_text_from_pdf_url(pdf_url, doi=None):\n",
    "    tried_urls = set()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    def fetch_unpaywall_pdf(doi):\n",
    "        if not doi:\n",
    "            return None\n",
    "        try:\n",
    "            unpaywall_url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "            r = requests.get(unpaywall_url, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                pdf_link = data.get(\"best_oa_location\", {}).get(\"url_for_pdf\")\n",
    "                if pdf_link:\n",
    "                    print(f\"üìñ Found Unpaywall PDF: {pdf_link}\")\n",
    "                    return pdf_link\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Unpaywall fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    def fetch_acm_pdf(doi):\n",
    "        if not doi:\n",
    "            return None\n",
    "        try:\n",
    "            acm_url = f\"https://dl.acm.org/doi/pdf/{doi}\"\n",
    "            r = requests.head(acm_url, allow_redirects=True, timeout=10)\n",
    "            if r.status_code == 200 and \"pdf\" in r.headers.get(\"Content-Type\", \"\").lower():\n",
    "                print(f\"üìÑ Found ACM PDF: {acm_url}\")\n",
    "                return acm_url\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è ACM fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    while pdf_url and pdf_url not in tried_urls:\n",
    "        tried_urls.add(pdf_url)\n",
    "        try:\n",
    "            response = requests.get(pdf_url, headers=headers, timeout=PDF_TIMEOUT)\n",
    "            content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
    "\n",
    "            if \"application/pdf\" in content_type:\n",
    "                try:\n",
    "                    pdf_bytes = io.BytesIO(response.content)\n",
    "                    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "                    text = \"\\n\\n\".join([page.get_text() for page in doc])\n",
    "                    if not text.strip():\n",
    "                        raise ValueError(\"No text extracted from PDF\")\n",
    "                    return text.strip(), pdf_url\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è PDF parse error with PyMuPDF: {e}\")\n",
    "                pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "                continue\n",
    "\n",
    "            if \"text/html\" in content_type:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                body_text = soup.get_text(separator=' ', strip=True).lower()\n",
    "                if any(x in body_text for x in [\"not found\", \"error 404\", \"no encontrado\", \"access denied\"]):\n",
    "                    pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "                    continue\n",
    "                text_labels = soup.select('[class*=\"textLayer\"], [id*=\"textLayer\"] div, span')\n",
    "                texts = [el.get_text(separator=' ', strip=True) for el in text_labels]\n",
    "                if texts:\n",
    "                    return \" \".join(texts), pdf_url\n",
    "                pdf_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.pdf')]\n",
    "                if pdf_links:\n",
    "                    next_pdf = requests.compat.urljoin(pdf_url, pdf_links[0])\n",
    "                    if next_pdf not in tried_urls:\n",
    "                        pdf_url = next_pdf\n",
    "                        continue\n",
    "                pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "                continue\n",
    "\n",
    "            pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Exception while fetching PDF: {e}\")\n",
    "            pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "\n",
    "    print(\"‚ùå No valid PDF or text found.\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate the number of tokens in a string for GPT models.\"\"\"\n",
    "    return len(ENCODING.encode(text))\n",
    "\n",
    "def analyze_text(instructions, pdf_text):\n",
    "    detected_languages = set()\n",
    "\n",
    "    # 1Ô∏è‚É£ Try LLM first\n",
    "    if pdf_text.strip():\n",
    "        prompt = f\"{instructions}\\n\\nText:\\n{pdf_text}\"\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"run\", MODEL_NAME],\n",
    "                input=prompt,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                encoding=\"utf-8\",\n",
    "                errors=\"ignore\"\n",
    "            )\n",
    "            raw = result.stdout.strip()\n",
    "            json_start = raw.find(\"{\")\n",
    "            json_end = raw.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                llm_result = json.loads(raw[json_start:json_end])\n",
    "                detected_languages.update(llm_result.get(\"programming_languages\", []))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return {\"programming_languages\": sorted(detected_languages)}\n",
    "\n",
    "# Make sure to set your API key in the environment\n",
    "# export OPENAI_API_KEY=\"sk-...\"\n",
    "def analyze_text_with_gpt(pdf_text, model=\"gpt-5-nano\"):\n",
    "    \"\"\"\n",
    "    Analyze PDF text using ChatGPT Responses API.\n",
    "    Returns a set of detected programming languages.\n",
    "    Automatically skips blocked content.\n",
    "    \"\"\"\n",
    "    detected_languages = set()\n",
    "\n",
    "    # Blocked content check\n",
    "    blocked_indicators = [\n",
    "        \"enable javascript and cookies to continue\",\n",
    "        \"access denied\",\n",
    "        \"not found\",\n",
    "        \"error 404\"\n",
    "    ]\n",
    "    preview_text = pdf_text[:300].replace(\"\\n\", \" \").lower()\n",
    "    if any(b in preview_text for b in blocked_indicators):\n",
    "        print(\"‚ö†Ô∏è Blocked content detected, skipping analysis.\")\n",
    "        return {\"programming_languages\": []}\n",
    "\n",
    "    # GPT-5 request\n",
    "    try:\n",
    "        response = openai.responses.create(\n",
    "            model=model,\n",
    "            input=f\"\"\"\n",
    "            You are a text analysis assistant specialized in identifying programming languages mentioned in academic or technical articles.\n",
    "\n",
    "            Task:\n",
    "            1Ô∏è‚É£ Identify **only actual programming languages** used to write code.\n",
    "            2Ô∏è‚É£ Do **NOT** include frameworks, libraries, standards, formal languages, or platforms.\n",
    "            3Ô∏è‚É£ Ignore any text after a ‚ÄúReferences‚Äù or ‚ÄúBibliography‚Äù section.\n",
    "            4Ô∏è‚É£ Return strictly in JSON:\n",
    "\n",
    "            {{\"programming_languages\": [\"Python\", \"C\", \"Java\"]}}\n",
    "\n",
    "            If none found, return:\n",
    "\n",
    "            {{\"programming_languages\": []}}\n",
    "\n",
    "\n",
    "            Text:\n",
    "            {pdf_text}\n",
    "\"\"\"\n",
    "        )\n",
    "        raw = response.output_text.strip()\n",
    "        json_start = raw.find(\"{\")\n",
    "        json_end = raw.rfind(\"}\") + 1\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            llm_result = json.loads(raw[json_start:json_end])\n",
    "            detected_languages.update(llm_result.get(\"programming_languages\", []))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è GPT analysis failed: {e}\")\n",
    "\n",
    "    return {\"programming_languages\": sorted(detected_languages)}\n",
    "\n",
    "# ----------------------\n",
    "# Main loop\n",
    "# ----------------------\n",
    "def process_all_obras():\n",
    "    obras = read_obras_from_csv(OBRAS_CSV)\n",
    "    print(f\"Found {len(obras)} obras in CSV.\")\n",
    "\n",
    "    next_tecn_id = init_csv(TECN_CSV, headers=[\"id\",\"nombre\"])\n",
    "    next_link_id = init_csv(OBRA_TECN_CSV, headers=[\"id\",\"obra_id\",\"tecnologia_id\"])\n",
    "\n",
    "    for obra_id, pdf_url, doi in obras:\n",
    "        print(f\"\\nüîπ Processing Obra ID: {obra_id}\")\n",
    "        try:\n",
    "            # Step 1: fetch PDF text\n",
    "            text, final_url = get_text_from_pdf_url(pdf_url, doi)\n",
    "            if not text:\n",
    "                print(f\"‚ùå No valid PDF or text found.\")\n",
    "                print(f\"‚ö†Ô∏è Skipping Obra ID {obra_id}, no text extracted.\")\n",
    "                continue\n",
    "            else:\n",
    "                preview = text[:300].replace(\"\\n\", \" \").strip()\n",
    "                print(f\"üìÑ Text extracted for Obra ID {obra_id} ({len(text)} chars)\")\n",
    "                print(f\"üîó Source URL used: {final_url}\")\n",
    "                print(f\"üìù Text preview: {preview}{'...' if len(text) > 300 else ''}\")\n",
    "\n",
    "            # Step 2: analyze with Ollama\n",
    "            print(f\"ü§ñ Analyzing text for Obra ID {obra_id}...\")\n",
    "            try:\n",
    "                result = analyze_text_with_gpt(text)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Analysis failed for Obra ID {obra_id}: {e}\")\n",
    "                result = {\"programming_languages\": []}\n",
    "\n",
    "            languages = result.get(\"programming_languages\", [])\n",
    "            print(f\"üìù Obra ID {obra_id} languages detected: {languages}\")\n",
    "\n",
    "            tech_map = load_tecnologias(TECN_CSV)  # { \"Python\": 89, \"C\": 90, ... }\n",
    "\n",
    "            for lang in languages:\n",
    "                if lang not in tech_map:\n",
    "                    tech_map[lang] = next_tecn_id\n",
    "                    append_to_csv(TECN_CSV, [next_tecn_id, lang], headers=[\"id\",\"nombre\"])\n",
    "                    next_tecn_id += 1\n",
    "\n",
    "                tecnologia_id = tech_map[lang]\n",
    "                append_to_csv(OBRA_TECN_CSV, [next_link_id, obra_id, tecnologia_id], headers=[\"id\",\"obra_id\",\"tecnologia_id\"])\n",
    "                next_link_id += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Unexpected error processing Obra ID {obra_id}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Uncomment if you are testing runs \n",
    "    \"\"\"for csv_file in [TECN_CSV, OBRA_TECN_CSV]:\n",
    "        if os.path.exists(csv_file):\n",
    "            os.remove(csv_file)\n",
    "            print(f\"üóëÔ∏è Deleted old CSV: {csv_file}\")\"\"\"\n",
    "    process_all_obras()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1701b",
   "metadata": {},
   "source": [
    "### ALMACENAMIENTO EN POSTGRESQL\n",
    "---\n",
    "\n",
    "Una vez declaradas las tablas en **PostgreSQL**, y los datos obtenidos mediante **OpenAlex**, y la API de **OpenAI**, y guardados en archivos *.csv (en la carpeta **cache**), procedemos a trasladar todos los datos guardados en esos archivos y almacenarlos en nuestra base de datos.\n",
    "\n",
    "Con los csvs llenos y PostgreSQL (por docker compose) conectado y con la BD creada, lo siguiente es llenarla. Como en el algoritmo de creaci√≥n de la BD, es necesario pasar los par√°metros de la base de datos para realizar la conexi√≥n con PostgreSQL: **host**, **port**, **database**, **user** y **password**. Con esta, ya tenemos el cursosr y podemos empezar con el proceso.\n",
    "\n",
    "La idea aqu√≠ es extraer los datos de los csvs con **Dataframes de pandas** y con estos hacer las distintas consultas para insertar (_INSERT into ..._) los dataframes en la BD. Por √∫ltimo, se realiza el commit y se cierra el cursosr y la conexi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "DB_PARAMS = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"demoDB\",\n",
    "    \"user\": \"userPSQL\",\n",
    "    \"password\": \"passPSQL\"\n",
    "}\n",
    "\n",
    "def main():\n",
    "    connection = psycopg2.connect(**DB_PARAMS)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    dir_cache = os.path.join(script_dir, '../cache')\n",
    "\n",
    "    # File paths\n",
    "    file_tematica = os.path.join(dir_cache, 'tematica.csv')\n",
    "    file_tematica_contenida = os.path.join(dir_cache, 'tematica_contenida.csv')\n",
    "    file_obra = os.path.join(dir_cache, 'obra.csv')\n",
    "    file_tecnologia = os.path.join(dir_cache, 'tecnologia.csv')\n",
    "    file_obra_tecnologia = os.path.join(dir_cache, 'obra_tecnologia.csv')\n",
    "\n",
    "    # Read CSVs\n",
    "    df_tematica = pd.read_csv(file_tematica)\n",
    "    df_tematica_contenida = pd.read_csv(file_tematica_contenida)\n",
    "    df_obra = pd.read_csv(file_obra)\n",
    "    df_tecnologia = pd.read_csv(file_tecnologia)\n",
    "    df_obra_tecnologia = pd.read_csv(file_obra_tecnologia)\n",
    "\n",
    "    # Insert tematica\n",
    "    for _, row in df_tematica.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tematica (id, nombre_campo)\n",
    "            VALUES (%s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (int(row['id']), row['nombre_campo'].strip() if pd.notna(row['nombre_campo']) else None))\n",
    "\n",
    "    # Insert tematica_contenida\n",
    "    for _, row in df_tematica_contenida.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tematica_contenida (id, tematica_padre_id, tematica_hijo_id)\n",
    "            VALUES (%s, %s, %s)\n",
    "            ON CONFLICT DO NOTHING\n",
    "        \"\"\", (int(row['id']), int(row['id_padre']), int(row['id_hijo'])))\n",
    "\n",
    "    # Insert obra\n",
    "    for _, row in df_obra.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO obra (\n",
    "                id, doi, direccion_fuente, titulo, abstract, fecha_publicacion,\n",
    "                idioma, num_citas, fwci, tematica_id\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (\n",
    "            int(row['id']),\n",
    "            row.get('doi').strip() if pd.notna(row.get('doi')) else None,\n",
    "            row['direccion_fuente'].strip() if pd.notna(row.get('direccion_fuente')) else None,\n",
    "            row['titulo'].strip() if pd.notna(row.get('titulo')) else None,\n",
    "            row.get('abstract').strip() if pd.notna(row.get('abstract')) else None,\n",
    "            row.get('fecha_publicacion') if pd.notna(row.get('fecha_publicacion')) else None,\n",
    "            row.get('idioma').strip() if pd.notna(row.get('idioma')) else None,\n",
    "            int(row.get('num_citas', 0)) if pd.notna(row.get('num_citas')) else 0,\n",
    "            float(row.get('fwci', 0.0)) if pd.notna(row.get('fwci')) else 0.0,\n",
    "            int(row.get('tematica_id')) if pd.notna(row.get('tematica_id')) else None\n",
    "        ))\n",
    "\n",
    "    # Insert tecnologia\n",
    "    for _, row in df_tecnologia.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tecnologia (id, nombre, tipo, version)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (\n",
    "            int(row['id']),\n",
    "            row['nombre'].strip() if pd.notna(row['nombre']) else None,\n",
    "            row.get('tipo').strip() if pd.notna(row.get('tipo')) else None,\n",
    "            row.get('version').strip() if pd.notna(row.get('version')) else None\n",
    "        ))\n",
    "\n",
    "    # Insert obra_tecnologia\n",
    "    for _, row in df_obra_tecnologia.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO obra_tecnologia (id, obra_id, tecnologia_id)\n",
    "            VALUES (%s, %s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (int(row['id']), int(row['obra_id']), int(row['tecnologia_id'])))\n",
    "# Puede que on conflict falle por lo de id \n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"‚úÖ CSV data loaded successfully including tecnologia and obra_tecnologia.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dadfffb",
   "metadata": {},
   "source": [
    "### Creaci√≥n del Grafo: Turtle, Schema.org & SKOS\n",
    "\n",
    "#### Mapeo de Entidades (PostgreSQL ‚Üí Vocabularios)\n",
    "\n",
    "En la √∫ltima entrega (Pr√°ctica 3) se defini√≥ un conjunto de mapeos entre las tablas de la base de datos _PostgreSQL_ y las clases de los vocabularios **SCHEMA** y **SKOS**.  \n",
    "Estos mapeos permiten representar las entidades relacionales (_obra_, _tem√°tica_, _tecnolog√≠a_, _obra_tecnolog√≠a_ y _tem√°tica_contenida_) como recursos sem√°nticos dentro de un grafo RDF.\n",
    "\n",
    "#### Generaci√≥n del Grafo RDF\n",
    "\n",
    "El siguiente paso consiste en **transformar los datos de la base de datos en un grafo RDF** y **exportarlos al formato Turtle (.ttl)**.  \n",
    "Este archivo podr√° ser cargado posteriormente en **GraphDB**, donde se podr√°n ejecutar consultas **SPARQL** sobre la informaci√≥n enlazada.\n",
    "\n",
    "El proceso se realiza mediante un script en **Python** que:\n",
    "\n",
    "1. Conecta con la base de datos PostgreSQL.  \n",
    "2. Itera sobre cada fila de las tablas relevantes.  \n",
    "3. Genera triples RDF utilizando las ontolog√≠as **SCHEMA** y **SKOS**.  \n",
    "4. Exporta el resultado final a un archivo `.ttl`.\n",
    "\n",
    "#### Descripci√≥n del Script\n",
    "\n",
    "El script realiza los siguientes pasos principales:\n",
    "\n",
    "- **Conexi√≥n a la base de datos** PostgreSQL mediante `psycopg2`.  \n",
    "- **Creaci√≥n de un grafo RDF** usando la librer√≠a `rdflib`.  \n",
    "- **Asignaci√≥n de namespaces**:  \n",
    "  - `schema:` ‚Üí [https://schema.org/](https://schema.org/)  \n",
    "  - `skos:` ‚Üí [https://www.w3.org/2004/02/skos/core#](https://www.w3.org/2004/02/skos/core#)  \n",
    "  - `openalex:` ‚Üí [https://openalex.org/](https://openalex.org/)  \n",
    "- **Iteraci√≥n sobre las tablas**:\n",
    "  - `tematica` ‚Üí Clases `skos:Concept`  \n",
    "  - `tematica_contenida` ‚Üí Relaciones jer√°rquicas `skos:broader` y `skos:narrower`  \n",
    "  - `tecnologia` ‚Üí Clases `schema:SoftwareApplication`  \n",
    "  - `obra` ‚Üí Clases `schema:TechArticle`  \n",
    "  - `obra_tecnologia` ‚Üí Relaciones `schema:mentions`  \n",
    "- **Exportaci√≥n del grafo** en formato Turtle (`.ttl`)  \n",
    "- **Cierre de la conexi√≥n** a la base de datos.\n",
    "\n",
    "#### Resultado Final\n",
    "\n",
    "El script genera un archivo RDF llamado **`openalex_graph.ttl`** dentro del directorio `ttl/`.  \n",
    "Este archivo contiene todos los triples RDF generados a partir de la base de datos y puede cargarse directamente en **GraphDB**.\n",
    "\n",
    "Una vez cargado, es posible ejecutar consultas **SPARQL** para:\n",
    "\n",
    "- Explorar relaciones entre obras y tecnolog√≠as.  \n",
    "- Visualizar jerarqu√≠as tem√°ticas mediante SKOS.  \n",
    "- Analizar la estructura sem√°ntica del conocimiento extra√≠do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004db1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import psycopg2\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, SKOS, XSD\n",
    "\n",
    "# --- Namespaces ---\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "OPENALEX = Namespace(\"https://openalex.org/\")\n",
    "g = Graph()\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"skos\", SKOS)\n",
    "g.bind(\"openalex\", OPENALEX)\n",
    "\n",
    "# --- PostgreSQL connection ---\n",
    "DB_PARAMS = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"demoDB\",\n",
    "    \"user\": \"userPSQL\",\n",
    "    \"password\": \"passPSQL\"\n",
    "}\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(**DB_PARAMS)\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Connected to database ‚úÖ\")\n",
    "\n",
    "# --- 1. TEM√ÅTICA ---\n",
    "cur.execute(\"SELECT id, nombre_campo FROM tematica;\")\n",
    "for tmid, nombre in cur.fetchall():\n",
    "    tema_uri = OPENALEX[f\"tematica_{tmid}\"]\n",
    "    g.add((tema_uri, RDF.type, SKOS.Concept))\n",
    "    g.add((tema_uri, SKOS.prefLabel, Literal(nombre)))\n",
    "\n",
    "print(\"Mapped table: tematica ‚úÖ\")\n",
    "\n",
    "# tematica_contenida ‚Üí skos:broader\n",
    "cur.execute(\"SELECT id, tematica_padre_id, tematica_hijo_id FROM tematica_contenida;\")\n",
    "for tcid, parent, child in cur.fetchall():\n",
    "    parent_uri = OPENALEX[f\"tematica_{parent}\"]\n",
    "    child_uri = OPENALEX[f\"tematica_{child}\"]\n",
    "    g.add((parent_uri, SKOS.narrower, child_uri))\n",
    "    g.add((child_uri, SKOS.broader, parent_uri))\n",
    "\n",
    "# --- 2. TECNOLOG√çA ---\n",
    "cur.execute(\"SELECT id, nombre, tipo, version FROM tecnologia;\")\n",
    "for tid, nombre, tipo, version in cur.fetchall():\n",
    "    tech_uri = OPENALEX[f\"tecnologia_{tid}\"]\n",
    "    g.add((tech_uri, RDF.type, SCHEMA.SoftwareApplication))\n",
    "    g.add((tech_uri, SCHEMA.name, Literal(nombre)))\n",
    "    if tipo:\n",
    "        g.add((tech_uri, SCHEMA.applicationCategory, Literal(tipo)))\n",
    "    if version:\n",
    "        g.add((tech_uri, SCHEMA.softwareVersion, Literal(version)))\n",
    "\n",
    "print(\"Mapped table: tecnologia ‚úÖ\")\n",
    "\n",
    "# --- 3. OBRA ---\n",
    "cur.execute(\"SELECT id, doi, direccion_fuente, titulo, abstract, fecha_publicacion, idioma, num_citas, fwci, tematica_id FROM obra;\")\n",
    "for oid, doi, direccion_fuente, titulo, abstract, fecha_publicacion, idioma, num_citas, fwci, tematica_id in cur.fetchall():\n",
    "    obra_uri = OPENALEX[f\"obra_{oid}\"]\n",
    "    g.add((obra_uri, RDF.type, SCHEMA.TechArticle))\n",
    "    if doi:\n",
    "        g.add((obra_uri, SCHEMA.sameAs, Literal(doi)))\n",
    "    if direccion_fuente:\n",
    "        g.add((obra_uri, SCHEMA.url, Literal(direccion_fuente)))\n",
    "    if titulo:\n",
    "        g.add((obra_uri, SCHEMA.name, Literal(titulo)))\n",
    "    if abstract:\n",
    "        g.add((obra_uri, SCHEMA.abstract, Literal(abstract)))\n",
    "    if fecha_publicacion:\n",
    "        g.add((obra_uri, SCHEMA.datePublished, Literal(fecha_publicacion, datatype=XSD.date)))\n",
    "    if idioma:\n",
    "        g.add((obra_uri, SCHEMA.inLanguage, Literal(idioma)))\n",
    "    if num_citas:\n",
    "        g.add((obra_uri, SCHEMA.citationCount, Literal(num_citas, datatype=XSD.integer)))\n",
    "    if fwci:\n",
    "        g.add((obra_uri, SCHEMA.metric, Literal(fwci, datatype=XSD.float)))\n",
    "    if tematica_id:\n",
    "        g.add((obra_uri, SCHEMA.about, OPENALEX[f\"tematica_{tematica_id}\"]))\n",
    "\n",
    "print(\"Mapped table: obra ‚úÖ\")\n",
    "\n",
    "# obra_tecnologia ‚Üí schema:mentions\n",
    "cur.execute(\"SELECT obra_id, tecnologia_id FROM obra_tecnologia;\")\n",
    "for oid, tid in cur.fetchall():\n",
    "    g.add((OPENALEX[f\"obra_{oid}\"], SCHEMA.mentions, OPENALEX[f\"tecnologia_{tid}\"]))\n",
    "\n",
    "\n",
    "print(\"Mapped relationships ‚úÖ\")\n",
    "\n",
    "# --- 4. EXPORT ---\n",
    "output_file = \"ttl/openalex_graph.ttl\"\n",
    "g.serialize(destination=output_file, format=\"turtle\")\n",
    "print(f\"RDF graph exported to {output_file} üß©\")\n",
    "\n",
    "# --- 5. Cleanup ---\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"PostgreSQL connection closed üîí\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ebbb30",
   "metadata": {},
   "source": [
    "### CREACI√ìN DE CONSULTAS\n",
    "---\n",
    "\n",
    "Para finalizar el proyecto, hemos creado una serie de querys con las que se pueda analizar s√≠ el sistema cumple los objetivos programados.\n",
    "\n",
    "Primeramente, se prob√≥ si cumpl√≠a el objetivo principal: obtener el n√∫mero de apariciones de distintos lenguajes de programaci√≥n en obras de cada subtem√°tica (o tem√°tica sin hijas) distinta. El resultado fue una tabla donde se listaban todas las combinaciones de t√≥pico y lenguaje distinto dentro de la base de datos, junto al n√∫mero de veces que se repet√≠a esa relaci√≥n. Por lo tanto, se cumpli√≥ la meta descrita al inicio del proyecto.\n",
    "\n",
    "Para demostrar su funcionamiento, solo hay que utilizar la siguiente consulta en GraphDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PREFIX schema: <https://schema.org/>\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "\n",
    "# Queremos una lista de \"aristas\" (conexiones)\n",
    "SELECT ?topicName ?techName (COUNT(DISTINCT ?work) AS ?sharedWorksCount)\n",
    "WHERE {\n",
    "    ?work schema:about ?topic .\n",
    "    ?work schema:mentions ?technology .\n",
    "    \n",
    "    OPTIONAL { ?topic skos:prefLabel ?topicName . }\n",
    "    OPTIONAL { ?technology schema:name ?techName . }\n",
    "    \n",
    "    FILTER(BOUND(?topicName) && BOUND(?techName))\n",
    "}\n",
    "GROUP BY ?topicName ?techName\n",
    "ORDER BY DESC(?sharedWorksCount)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8cb0d8",
   "metadata": {},
   "source": [
    "Posteriormente, para exprimir los l√≠mites del sistema, decidimos probar una consulta m√°s: Tecnolog√≠as m√°s competitivas entre campos. En esta consulta, se lista el n√∫mero de veces que cada par de tecnolog√≠as distintas (por ejemplo, Python y Java, C++ y Prolog, etc.) aparecen en las mismas obras. Con el resultado, aprendimos que Python y Java eran la dupla que m√°s aparec√≠a en las mismas obras.\n",
    "\n",
    "Para demostrar su funcionamiento, solo hay que utilizar la siguiente consulta en GraphDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PREFIX schema: <https://schema.org/>\n",
    "\n",
    "SELECT ?techA_name ?techB_name (COUNT(DISTINCT ?topic) AS ?commonTopics)\n",
    "WHERE {\n",
    "    # Encuentra la primera tecnolog√≠a (A) mencionada por una obra\n",
    "    ?workA schema:mentions ?techA .\n",
    "    ?workA schema:about ?topic .\n",
    "    ?techA schema:name ?techA_name .\n",
    "\n",
    "    # Encuentra la segunda tecnolog√≠a (B) mencionada por la misma obra (o una obra sobre el mismo tema)\n",
    "    ?workB schema:mentions ?techB .\n",
    "    ?workB schema:about ?topic . # <-- Mismo tema\n",
    "    ?techB schema:name ?techB_name .\n",
    "\n",
    "    # Asegura que no sea la misma tecnolog√≠a\n",
    "    FILTER (?techA != ?techB)\n",
    "\n",
    "    # Solo queremos los nombres, no las URIs largas\n",
    "    FILTER(BOUND(?techA_name) && BOUND(?techB_name))\n",
    "}\n",
    "GROUP BY ?techA_name ?techB_name\n",
    "ORDER BY DESC(?commonTopics)\n",
    "LIMIT 10\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce5acd",
   "metadata": {},
   "source": [
    "### PARTICIPANTES:\n",
    "* Jaime Vaquero Rabahieh. Correo: jaime.vaquero@alumnos.upm.es\n",
    "* Zakaria Lasry Sahraoui. Correo: z.lsahraoui@alumnos.upm.es\n",
    "* Damian Sanchez Maqueda. Correo: damian.sanchez@alumnos.upm.es\n",
    "* Radu-Andrei Bourceanu. Correo: r.bourceanu@alumnos.upm.es"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
