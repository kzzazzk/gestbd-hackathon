{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18dfe24",
   "metadata": {},
   "source": [
    "## GESTDB HACKATON GRUPO 1: OPENALEX\n",
    "\n",
    "### PARTICIPANTES:\n",
    "* Jaime Vaquero Rabahieh. Correo: jaime.vaquero@alumnos.upm.es\n",
    "* Zakaria Lasry Sahraoui. Correo: z.lsahraoui@alumnos.upm.es\n",
    "* Damian Sanchez Maqueda. Correo: damian.sanchez@alumnos.upm.es\n",
    "* Radu-Andrei Bourceanu. Correo: r.bourceanu@alumnos.upm.es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c8d4f",
   "metadata": {},
   "source": [
    "## PLANTEAMIENTO Y ACOTACI√ìN DE OBJETVIOS INICIALES\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac50be",
   "metadata": {},
   "source": [
    "El prop√≥sito inicial busca responder a qu√© instituciones y pa√≠ses son los \"hubs\" de conocimiento actuales. Para ello, se analizar√° cu√°les colaboran m√°s con otros centros y publican m√°s sobre una tem√°tica concreta.\n",
    "\n",
    "Las problem√°ticas que hallamos con esta elecci√≥n eran la siguientes:\n",
    "\n",
    "* No utilizabamos ninguna fuente de datos no estructurados: Optamos por dirigir el proyecto a un enfoque diferente que parte del inter√©s de comprender que tecnolog√≠as de programaci√≥n son las mas usadas, en art√≠culos o proyectos de investigaci√≥n relacionados con la computaci√≥n, y como var√≠a su uso y distribuci√≥n en funci√≥n de cada subcampo. En concreto en este proyecto nos hemos centrado en la en √°rea de la Inteligencia Artifical, por ser la que tiene mayor relaci√≥n con el mater que actualmente nos encontramos cursando.\n",
    "\n",
    "\n",
    "A continuaci√≥n se muestra la imagen de la estructura completa de la base de datos del proyecto.\n",
    "\n",
    "![Diagrama Entidad-Relaci√≥n del sistema](./images/relaciones%20db.png)\n",
    "\n",
    "\n",
    "* La cantidad de PDF a gestionar era demasiado amplia: \n",
    "OpenAlex cuenta con un total de **200 Millones de archivos**, lo cual implicar√≠a una cantidad alarmante de tiempo para extraer y procesar toda la informaci√≥n necesaria para cumplir el objetivo. Por lo tanto, decidimos utilizar el campo _Tem√°tica_ para filtrar el n√∫mero de archivos a extraer. Para poder obtener nuestro objetivo (extraer tecnolog√≠as por tema), decidimos filtrar una sucesi√≥n de tem√°ticas padre-hijo relacionadas con nuestra meta: ***Computer_Science -> Artificial Inteligence -> Tem√°ticas sin hijo***. Tambi√©n fue necesario filtrar por archivos con Pdf y que puedan ser compatibles con el LLM (OpenAI). Como consecuencia, el n√∫mero de obras a extraer se reduci√≥ a 6000. La siguiente imagen es una representaci√≥n de dicho filtro dentro de OpenAlex:\n",
    "\n",
    "![Filtro tem√°ticas dentro de OpenAlex](./images/filtro_openalex.jpeg)\n",
    "\n",
    "En base a esta decisi√≥n adem√°s decidimos acotar el dominio de entidades para abordar lo suficiente para hacer una demostraci√≥n en el Hackathon:\n",
    "\n",
    "![Arquitectura reducida del sistema](./images/esquema_redux.jpeg)\n",
    "\n",
    "## Especificaciones t√©cnicas por fase\n",
    "\n",
    "### Datos estructurados\n",
    "Mediante **OpenAlex**, una base de datos abierta de investigaciones cient√≠ficas a nivel global, similar a Google Scholar, pero con datos accesibles mediante API, podemos extraer articulos academicos, incluyendo metadatos como su direcci√≥n fuente, abstract, numero de citas, autores e instituciones asociadas al autor en el momento de la publicaci√≥n.\n",
    "\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "> Andrea Cimmino public√≥ un art√≠culo durante su estancia en la UPM, universidad espa√±ola y europea, sobre NLP y siendo este desarrollado principalmente en Python\n",
    "\n",
    "\n",
    "Usando la biblioteca `requests` para hacer llamadas a la **API de OpenAlex**, extrayendo as√≠ el contenido descrito y almacenarlo en una base de datos de **PostgreSQL**.\n",
    "\n",
    "\n",
    "\n",
    "### Datos no estructurados\n",
    "\n",
    "Sobre esta base de datos estructurada, aplicamos un proceso de an√°lisis de texto mediante la API de **OpenAI** para identificar para cada obra cual ha sido la tecnolog√≠a utilizada. Esto lo hemos realizado pasandole a `gpt-5-nano` la url de cada obra para que as√≠ pueda identificar la tecnolog√≠a usada sin necesidad de guardar una copia de cada obra en la base de datos. Todas la tecnolog√≠as conocidas por nuestra base de datos se almacenan como una entidad, y se relacionan directamente con las obras en la que el modelo haya detectado su uso la misma. Esta estructura permite explorar facilmente tendencias tecnol√≥gicas y la difusi√≥n de herramientas de programaci√≥n dentro del √°mbito cient√≠fico.\n",
    "\n",
    "### Datos enlazados\n",
    "\n",
    "\n",
    "\n",
    "Una vez terminado el procesamiento de los documentos y habiendo extra√≠do las tecnolog√≠as de cada obra est√°s insertadas tambi√©n a la BBDD **PostgreSQL**, para construir el grafo de nuestro sistema, hemos utilizado **GraphDB** y as√≠ poder realizar las consultas necesarias del obetivo principal. Para facilitar este proceso hemos utilizado **RDF Turtle**.\n",
    "\n",
    "Con el prop√≥sito de enriquecer sem√°nticamente los datos extra√≠dos y facilitar su interoperabilidad con otras fuentes abiertas, hemos seleccionado diversos vocabularios, ontolog√≠as y grafos de conocimiento ampliamente utilizados en el ecosistema de la web sem√°ntica. Cada uno de ellos contribuye a describir un tipo de entidad diferente dentro del modelo propuesto (obras, autores, instituciones o lenguajes de programaci√≥n). El modelo combina informaci√≥n estructurada con elementos sem√°nticos, utilizando Schema.org para describir los lenguajes y tecnolog√≠as detectadas, y SKOS para organizar jer√°rquicamente las entidades del dominio (obras, temas, etc.), lo que facilita la interoperabilidad y la consulta avanzada de los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd83b5f",
   "metadata": {},
   "source": [
    "## TRABAJO REALIZADO EN EL HACKATHON\n",
    "\n",
    "### CREACI√ìN BASE DE DATOS POSTGRESQL\n",
    "---\n",
    "\n",
    "Y ya con el sistema filtrado, pudimos empezar a programarlo. Lo primero fue crear la base de datos en PostgreSQL, para lo cual declaramos los parametros de la BD (**host**, **port**, **database**, **user** y **password**) para lrealizar la conexi√≥n y obtener el cursosr. Posteriormente, escribimos en _tablas SQL_ las entidades _Obra_, _Tecnologia_ y _Tematica_ y las relaciones entre tem√°ticas padre e hijo (*Tem√°tica_contenida*) y entre obras y tecnolog√≠as (*obra_tecnolog√≠a*) Cabe destacar que la relaci√≥n entre _Obra_ y _Tematica_ se encuentra representada por la Foreign Key *tecnologia_id*. Adem√°s, se crean √≠ndices externos para ayudar a la hora de organizar los datos en las tablas.\n",
    "\n",
    "Por √∫ltimo, el sistema se conecta con el servidor de PostgreSQL para crear las tablas, por lo que es necesario **lanzar antes la imagen de docker de PostgreSQL para que funcione**. Si todo sale bien, al final se hace un commit con los cmabios y se cierran el ursor y la conexi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cfe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Database 'demoDB' already exists.\n",
      "‚úÖ Tables created or verified successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "DB_PARAMS = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"demoDB\",\n",
    "    \"user\": \"userPSQL\",\n",
    "    \"password\": \"passPSQL\"\n",
    "}\n",
    "\n",
    "sql_script = \"\"\"CREATE TABLE IF NOT EXISTS tematica (\n",
    "    id INTEGER,\n",
    "    nombre_campo TEXT NOT NULL,\n",
    "        PRIMARY KEY (id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS tecnologia (\n",
    "    id INTEGER,\n",
    "    nombre TEXT NOT NULL,\n",
    "    tipo TEXT,\n",
    "    version TEXT,\n",
    "    PRIMARY KEY (id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS obra (\n",
    "    id INTEGER,\n",
    "    doi TEXT UNIQUE,  -- NEW\n",
    "    direccion_fuente TEXT NOT NULL,\n",
    "    titulo TEXT NOT NULL,\n",
    "    abstract TEXT,\n",
    "    fecha_publicacion TEXT,\n",
    "    idioma TEXT,\n",
    "    num_citas INTEGER DEFAULT 0,\n",
    "    fwci REAL,\n",
    "    tematica_id INTEGER,\n",
    "    PRIMARY KEY (id),\n",
    "    FOREIGN KEY (tematica_id)\n",
    "        REFERENCES tematica(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE SET NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS obra_tecnologia (\n",
    "    id INTEGER,\n",
    "    obra_id INTEGER NOT NULL,\n",
    "    tecnologia_id INTEGER NOT NULL,\n",
    "    PRIMARY KEY (id),\n",
    "    FOREIGN KEY (obra_id)\n",
    "        REFERENCES obra(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE CASCADE,\n",
    "    FOREIGN KEY (tecnologia_id)\n",
    "        REFERENCES tecnologia(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE RESTRICT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS tematica_contenida (\n",
    "    id INTEGER,\n",
    "    tematica_padre_id INTEGER NOT NULL,\n",
    "    tematica_hijo_id INTEGER NOT NULL,\n",
    "    PRIMARY KEY (id),\n",
    "    FOREIGN KEY (tematica_padre_id)\n",
    "        REFERENCES tematica(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE CASCADE,\n",
    "    FOREIGN KEY (tematica_hijo_id)\n",
    "        REFERENCES tematica(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE CASCADE,\n",
    "    CHECK (tematica_padre_id <> tematica_hijo_id)\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_obra_tematica ON obra(tematica_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_obratec_tecnologia ON obra_tecnologia(tecnologia_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_tematica_hijo ON tematica_contenida(tematica_hijo_id);\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    # Connect to default database to check/create demoDB\n",
    "    connection = psycopg2.connect(\n",
    "        host=DB_PARAMS['host'],\n",
    "        port=DB_PARAMS['port'],\n",
    "        database=\"demoDB\",\n",
    "        user=DB_PARAMS['user'],\n",
    "        password=DB_PARAMS['password']\n",
    "    )\n",
    "    connection.autocommit = True\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT 1 FROM pg_database WHERE datname = %s;\", (DB_PARAMS[\"database\"],))\n",
    "    exists = cursor.fetchone()\n",
    "\n",
    "    if not exists:\n",
    "        cursor.execute(sql.SQL(f\"CREATE DATABASE {DB_PARAMS['database']};\"))\n",
    "        print(f\"‚úÖ Database '{DB_PARAMS['database']}' created.\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Database '{DB_PARAMS['database']}' already exists.\")\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    # Connect to demoDB to create tables\n",
    "    connection = psycopg2.connect(**DB_PARAMS)\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(sql_script)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"‚úÖ Tables created or verified successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962917fe",
   "metadata": {},
   "source": [
    "### OBTENCI√ìN DATOS ESTRUCTURADOS (REQUEST)\n",
    "---\n",
    "\n",
    "Aunque nuestra idea siempre fue el uso de la librer√≠a _pyalex_ para extraer los datos estructurados, al final decidimos cambiarla por _requests_ ya que _pyalex_ daba problemas al extraer datos con s√≠mbolos no alfanum√©ricos. Eso s√≠, se mantuvo la idea de crear los csvs de cada entidad (_Obra_, _Tecnologia_ y _Tematica_) y tambi√©n para las relaciones con tabla en PostGreSQL (*obra_tecnologia* y *tematica_contenida*). Se ha creado una carpeta **Cache** para almacenar los csvs.\n",
    "\n",
    "La idea de este algoritmo es obtener los datos pedidos en cada entidad y relaci√≥n, despu√©s limpiarlos y por √∫ltimo organizarlos y almacenarlos en cada csv correspondiente. El propio algoritmo env√≠a a OpenAlex el filtro de b√∫squeda antes indicado y obtiene la informaci√≥n de todos los ficheros resultantes. Con cada fichero, obtiene solamente los datos que se relacionen con los distintos atributos de cada entidad y relaci√≥n creada y estos son limpiados y almacenados en el diccionario correspondiente al csv donde deben aparecer (puede haber datos como los _ids_ que pueden aparecer en m√°s de un csv). Por √∫ltimo, se crean los csvs si no existen y se guarda cada diccionario donde le correpsonde.\n",
    "\n",
    "Cabe destacar que la extarcci√≥n es por **p√°ginas de 200 entradas**. Por tanto, si llega a una p√°gina n√∫mero J donde _J x 200 > n√∫mero de papers detectados_, pues se termina la extracci√≥n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e722b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "PER_PAGE = 200\n",
    "KEYWORDS = [\n",
    "    \"python\",\"c-programming-language\",\"javascript\",\"java\",\"java-programming-language\",\n",
    "    \"sql\",\"dart\",\"swift\",\"cobol\",\"fortran\",\"matlab\",\"prolog\",\"lisp\",\"haskell\",\"rust\",\"perl\",\n",
    "    \"scala\",\"html\",\"html5\"\n",
    "]\n",
    "SUBFIELD_ID = \"subfields/1702\"\n",
    "LANGUAGE = \"languages/en\"\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # go up one level from src/\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
    "\n",
    "CSV_OBRA = os.path.join(CACHE_DIR, \"obra.csv\")\n",
    "CSV_TEMATICA = os.path.join(CACHE_DIR, \"tematica.csv\")\n",
    "CSV_TEMATICA_CONTENIDA = os.path.join(CACHE_DIR, \"tematica_contenida.csv\")\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "BASE_TOPICS = [\"Physical Sciences\", \"Computer Science\", \"Artificial Intelligence\"]\n",
    "\n",
    "def reconstruct_abstract(abstract_inverted_index):\n",
    "    if not abstract_inverted_index or not isinstance(abstract_inverted_index, dict):\n",
    "        return \"\"\n",
    "    position_map = {}\n",
    "    for word, positions in abstract_inverted_index.items():\n",
    "        for pos in positions:\n",
    "            position_map[pos] = word\n",
    "    return \" \".join(position_map[pos] for pos in sorted(position_map.keys()))\n",
    "\n",
    "def fetch_page(url, params, max_retries=5, delay_base=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            print(f\"‚ö†Ô∏è Warning: Bad response {response.status_code}, retrying...\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Exception during request: {e}\")\n",
    "        retries += 1\n",
    "        time.sleep(delay_base ** retries)\n",
    "    print(f\"‚ùå Error: Failed to fetch page after {max_retries} retries.\")\n",
    "    return None\n",
    "\n",
    "def initialize_csv_files():\n",
    "    os.makedirs(os.path.dirname(CSV_OBRA), exist_ok=True)\n",
    "    with open(CSV_OBRA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"id\",\"direccion_fuente\",\"titulo\",\"abstract\",\"fecha_publicacion\",\n",
    "            \"idioma\",\"num_citas\",\"fwci\",\"tematica_id\",\"doi\"\n",
    "        ])\n",
    "    print(f\"Initialized '{CSV_OBRA}' for writing works.\")\n",
    "\n",
    "def fetch_all_works():\n",
    "    tematica_map = {}\n",
    "    next_tematica_id = 1\n",
    "    obra_id = 1\n",
    "    page = 1\n",
    "    total_results = None\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"per_page\": PER_PAGE,\n",
    "            \"filter\": f\"open_access.is_oa:true,has_content.pdf:true,primary_topic.subfield.id:{SUBFIELD_ID},best_oa_location.is_accepted:true,language:{LANGUAGE},keywords.id:{'|'.join(KEYWORDS)}\",\n",
    "            \"sort\": \"cited_by_count:desc\"\n",
    "        }\n",
    "        data = fetch_page(BASE_URL, params)\n",
    "        if not data or \"results\" not in data:\n",
    "            print(f\"No data returned for page {page}, stopping.\")\n",
    "            break\n",
    "\n",
    "        works = data[\"results\"]\n",
    "        if total_results is None:\n",
    "            total_results = data.get(\"meta\", {}).get(\"count\", 0)\n",
    "            print(f\"Total results to fetch (approximate): {total_results}\")\n",
    "\n",
    "        print(f\"Fetched page {page} with {len(works)} works.\")\n",
    "\n",
    "        with open(CSV_OBRA, \"a\", newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for work in works:\n",
    "                # --- PDF URL ---\n",
    "                pdf_url = work.get(\"best_oa_location\", {}).get(\"pdf_url\")\n",
    "                if not pdf_url:\n",
    "                    continue\n",
    "\n",
    "                # --- DOI ---\n",
    "                doi = work.get(\"doi\", \"\")\n",
    "\n",
    "                titulo = work.get(\"title\", \"\")\n",
    "                abstract = reconstruct_abstract(work.get(\"abstract_inverted_index\"))\n",
    "                fecha_publicacion = work.get(\"publication_date\", \"\")\n",
    "                idioma = work.get(\"language\", LANGUAGE)\n",
    "                num_citas = work.get(\"cited_by_count\", 0)\n",
    "                fwci = work.get(\"fwci\", \"\")\n",
    "                primary_topic = work.get(\"primary_topic\")\n",
    "                if not primary_topic:\n",
    "                    continue\n",
    "                topic_name = primary_topic.get(\"display_name\", \"Unknown Topic\")\n",
    "                if topic_name not in tematica_map:\n",
    "                    tematica_map[topic_name] = next_tematica_id\n",
    "                    next_tematica_id += 1\n",
    "                tematica_id = tematica_map[topic_name]\n",
    "\n",
    "                writer.writerow([\n",
    "                    obra_id, pdf_url, titulo, abstract, fecha_publicacion,\n",
    "                    idioma, num_citas, fwci, tematica_id, doi\n",
    "                ])\n",
    "                obra_id += 1\n",
    "\n",
    "        if page * PER_PAGE >= total_results or not works:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    print(f\"Finished fetching all works. Total works saved: {obra_id-1}\")\n",
    "    return tematica_map\n",
    "\n",
    "def save_tematica_csv(tematica_map):\n",
    "    os.makedirs(os.path.dirname(CSV_TEMATICA), exist_ok=True)\n",
    "    with open(CSV_TEMATICA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"nombre_campo\"])\n",
    "        for topic_name, topic_id in tematica_map.items():\n",
    "            writer.writerow([topic_id, topic_name])\n",
    "    print(f\"Saved '{CSV_TEMATICA}' with {len(tematica_map)} topics.\")\n",
    "\n",
    "def update_tematica_and_generate_contenida():\n",
    "    if not os.path.exists(CSV_TEMATICA):\n",
    "        raise FileNotFoundError(f\"{CSV_TEMATICA} not found.\")\n",
    "\n",
    "    tematicas = {}\n",
    "    rows = []\n",
    "    with open(CSV_TEMATICA, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            row[\"id\"] = int(row[\"id\"])\n",
    "            rows.append(row)\n",
    "            tematicas[row[\"nombre_campo\"].strip()] = row[\"id\"]\n",
    "\n",
    "    max_id = max(r[\"id\"] for r in rows)\n",
    "    for topic in BASE_TOPICS:\n",
    "        if topic not in tematicas:\n",
    "            max_id += 1\n",
    "            tematicas[topic] = max_id\n",
    "            rows.append({\"id\": max_id, \"nombre_campo\": topic})\n",
    "            print(f\"Added base topic '{topic}' with id={max_id}\")\n",
    "\n",
    "    with open(CSV_TEMATICA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"id\", \"nombre_campo\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    relaciones = [\n",
    "        {\"id_padre\": tematicas[\"Physical Sciences\"], \"id_hijo\": tematicas[\"Computer Science\"]},\n",
    "        {\"id_padre\": tematicas[\"Computer Science\"], \"id_hijo\": tematicas[\"Artificial Intelligence\"]}\n",
    "    ]\n",
    "    ai_id = tematicas[\"Artificial Intelligence\"]\n",
    "    for nombre, id_ in tematicas.items():\n",
    "        if nombre not in BASE_TOPICS:\n",
    "            relaciones.append({\"id_padre\": ai_id, \"id_hijo\": id_})\n",
    "    relaciones = [{\"id_padre\": p, \"id_hijo\": h} for p, h in {(r[\"id_padre\"], r[\"id_hijo\"]) for r in relaciones}]\n",
    "\n",
    "    os.makedirs(os.path.dirname(CSV_TEMATICA_CONTENIDA), exist_ok=True)\n",
    "    with open(CSV_TEMATICA_CONTENIDA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"id_padre\", \"id_hijo\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(relaciones)\n",
    "\n",
    "    print(f\"'{CSV_TEMATICA}' updated with {len(rows)} topics.\")\n",
    "    print(f\"'{CSV_TEMATICA_CONTENIDA}' generated with {len(relaciones)} relations.\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting OpenAlex fetch process...\")\n",
    "    initialize_csv_files()\n",
    "    tematica_map = fetch_all_works()\n",
    "    save_tematica_csv(tematica_map)\n",
    "    update_tematica_and_generate_contenida()\n",
    "    print(\"Finished fetching and processing all works and topics.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd7bce",
   "metadata": {},
   "source": [
    "### OBTENCI√ìN DATOS NO ESTRUCTURADOS (LLM)\n",
    "---\n",
    "Los datos no estructurados en este sistema son las tecnolog√≠as dentro de las obras, las cuales deben extaerse manualmente desde los pdfs. Para ello, como OpenAlex tiene los _doi_ y los _url_ de las obras pero no su contenido como tal, ha sido necesario hacer _requests_ utilizando cada url de cada obra. Eso s√≠, seg√∫n el formato de estas, la extracci√≥n del texto var√≠a:\n",
    "\n",
    "- Si es un PDF, conseguimos el texto proces√°ndolo localmente.\n",
    "\n",
    "- Si es un PDF, pero su acceso est√° bloqueado o no se encuentra con la URL; probamos con unpaywall y acm con el doi del art√≠culo.\n",
    "\n",
    "- Si no es un PDF, comprobamos que no ha devuelto un error falso (un c√≥digo 200 en el que dice que no ha sido encontrado el archivo) y si nos lo ha devuelto probamos la query con unpaywall y acm con el doi del art√≠culo.\n",
    "\n",
    "\n",
    "- Si no es un PDF y no devuelve error falso iteramos por el textLabel del pdfviewer en html con _BeautifulSoup_ y scrapeamos el texto de ah√≠.\n",
    "\n",
    "Con el texto ya extraido, lo siguiente es la extracci√≥n de las tecnolog√≠as. Para ello, hemos utilizado un LLM local programado dentro del c√≥digo que utiliza **GPT 5.0 nano de OpenAI** para leer el c√≥digo y luego extraer el texto. Ya por √∫ltimo se a√±aden las tecnolog√≠as al csv de _Tecnologias_ y se crean las relaciones dentro de *obra_tecnologia* con el id de cada obra y el nuevo id generado para cada tecnolog√≠a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d558a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import json\n",
    "import openai\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz \n",
    "\n",
    "\n",
    "MODEL_NAME = \"mistral:instruct\"\n",
    "PDF_TIMEOUT = 30\n",
    "UNPAYWALL_EMAIL = \"your_email@example.com\"\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
    "OBRAS_CSV = os.path.join(CACHE_DIR, \"obra.csv\")\n",
    "TECN_CSV = os.path.join(CACHE_DIR, \"tecnologia.csv\")\n",
    "OBRA_TECN_CSV = os.path.join(CACHE_DIR, \"obra_tecnologia.csv\")\n",
    "\n",
    "\n",
    "instructions = \"\"\"You are a text analysis assistant specialized in identifying programming languages mentioned in academic or technical articles.\n",
    "Analyze the provided raw text (extracted directly from a PDF). Identify and return the main programming languages mentioned in the article (do not include frameworks, libraries, or tools).\n",
    "If a ‚ÄúReferences‚Äù or ‚ÄúBibliography‚Äù section appears, ignore all text after that marker.\n",
    "Return strictly in JSON, like:\n",
    "{\n",
    "  \"programming_languages\": [\"Python\", \"C\", \"Java\"]\n",
    "}\n",
    "If none found:\n",
    "{\n",
    "  \"programming_languages\": []\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------\n",
    "# CSV helpers\n",
    "# ----------------------\n",
    "def read_obras_from_csv(file_path):\n",
    "    obras = []\n",
    "    with open(file_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            obras.append((int(row[\"id\"]), row[\"direccion_fuente\"], row.get(\"doi\")))\n",
    "    return obras\n",
    "\n",
    "def init_csv(file_path, headers=None):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
    "        with open(file_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            if headers:\n",
    "                writer.writerow(headers)\n",
    "        return 1\n",
    "    max_id = 0\n",
    "    with open(file_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                val = row.get(\"id\")\n",
    "                if val:\n",
    "                    try:\n",
    "                        max_id = max(max_id, int(val))\n",
    "                    except:\n",
    "                        continue\n",
    "        except:\n",
    "            f.seek(0)\n",
    "            for line in f:\n",
    "                parts = line.split(\",\")\n",
    "                if parts:\n",
    "                    try:\n",
    "                        max_id = max(max_id, int(parts[0].strip()))\n",
    "                    except:\n",
    "                        continue\n",
    "    return max_id + 1\n",
    "\n",
    "def append_unique_to_csv(file_path, row, headers=None, key_index=1):\n",
    "    init_csv(file_path, headers=headers)\n",
    "    existing_keys = set()\n",
    "    with open(file_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        peek = next(reader, None)\n",
    "        if headers and peek and all(h in peek for h in headers):\n",
    "            pass\n",
    "        else:\n",
    "            if peek:\n",
    "                try:\n",
    "                    existing_keys.add(peek[key_index])\n",
    "                except:\n",
    "                    pass\n",
    "        for r in reader:\n",
    "            try:\n",
    "                existing_keys.add(r[key_index])\n",
    "            except:\n",
    "                continue\n",
    "    key = row[key_index] if len(row) > key_index else None\n",
    "    if key not in existing_keys:\n",
    "        with open(file_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "def append_to_csv(file_path, row, headers=None):\n",
    "    \"\"\"Simple append without uniqueness (for obra_tecnologia)\"\"\"\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    with open(file_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists and headers:\n",
    "            writer.writerow(headers)\n",
    "        writer.writerow(row)\n",
    "def load_tecnologias(file_path):\n",
    "    tech_map = {}\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                tech_map[row[\"nombre\"]] = int(row[\"id\"])\n",
    "    return tech_map\n",
    "\n",
    "# ----------------------\n",
    "# PDF + Analysis\n",
    "# ----------------------\n",
    "def get_text_from_pdf_url(pdf_url, doi=None):\n",
    "    tried_urls = set()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    def fetch_unpaywall_pdf(doi):\n",
    "        if not doi:\n",
    "            return None\n",
    "        try:\n",
    "            unpaywall_url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "            r = requests.get(unpaywall_url, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                pdf_link = data.get(\"best_oa_location\", {}).get(\"url_for_pdf\")\n",
    "                if pdf_link:\n",
    "                    print(f\"üìñ Found Unpaywall PDF: {pdf_link}\")\n",
    "                    return pdf_link\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Unpaywall fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_text_from_pdf_url(pdf_url, doi=None):\n",
    "    tried_urls = set()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    def fetch_unpaywall_pdf(doi):\n",
    "        if not doi:\n",
    "            return None\n",
    "        try:\n",
    "            unpaywall_url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "            r = requests.get(unpaywall_url, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                pdf_link = data.get(\"best_oa_location\", {}).get(\"url_for_pdf\")\n",
    "                if pdf_link:\n",
    "                    print(f\"üìñ Found Unpaywall PDF: {pdf_link}\")\n",
    "                    return pdf_link\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Unpaywall fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    def fetch_acm_pdf(doi):\n",
    "        if not doi:\n",
    "            return None\n",
    "        try:\n",
    "            acm_url = f\"https://dl.acm.org/doi/pdf/{doi}\"\n",
    "            r = requests.head(acm_url, allow_redirects=True, timeout=10)\n",
    "            if r.status_code == 200 and \"pdf\" in r.headers.get(\"Content-Type\", \"\").lower():\n",
    "                print(f\"üìÑ Found ACM PDF: {acm_url}\")\n",
    "                return acm_url\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è ACM fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    while pdf_url and pdf_url not in tried_urls:\n",
    "        tried_urls.add(pdf_url)\n",
    "        try:\n",
    "            response = requests.get(pdf_url, headers=headers, timeout=PDF_TIMEOUT)\n",
    "            content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
    "\n",
    "            if \"application/pdf\" in content_type:\n",
    "                try:\n",
    "                    pdf_bytes = io.BytesIO(response.content)\n",
    "                    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "                    text = \"\\n\\n\".join([page.get_text() for page in doc])\n",
    "                    if not text.strip():\n",
    "                        raise ValueError(\"No text extracted from PDF\")\n",
    "                    return text.strip(), pdf_url\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è PDF parse error with PyMuPDF: {e}\")\n",
    "                pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "                continue\n",
    "\n",
    "            if \"text/html\" in content_type:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                body_text = soup.get_text(separator=' ', strip=True).lower()\n",
    "                if any(x in body_text for x in [\"not found\", \"error 404\", \"no encontrado\", \"access denied\"]):\n",
    "                    pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "                    continue\n",
    "                text_labels = soup.select('[class*=\"textLayer\"], [id*=\"textLayer\"] div, span')\n",
    "                texts = [el.get_text(separator=' ', strip=True) for el in text_labels]\n",
    "                if texts:\n",
    "                    return \" \".join(texts), pdf_url\n",
    "                pdf_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.pdf')]\n",
    "                if pdf_links:\n",
    "                    next_pdf = requests.compat.urljoin(pdf_url, pdf_links[0])\n",
    "                    if next_pdf not in tried_urls:\n",
    "                        pdf_url = next_pdf\n",
    "                        continue\n",
    "                pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "                continue\n",
    "\n",
    "            pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Exception while fetching PDF: {e}\")\n",
    "            pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "\n",
    "    print(\"‚ùå No valid PDF or text found.\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate the number of tokens in a string for GPT models.\"\"\"\n",
    "    return len(ENCODING.encode(text))\n",
    "\n",
    "def analyze_text(instructions, pdf_text):\n",
    "    detected_languages = set()\n",
    "\n",
    "    # 1Ô∏è‚É£ Try LLM first\n",
    "    if pdf_text.strip():\n",
    "        prompt = f\"{instructions}\\n\\nText:\\n{pdf_text}\"\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"run\", MODEL_NAME],\n",
    "                input=prompt,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                encoding=\"utf-8\",\n",
    "                errors=\"ignore\"\n",
    "            )\n",
    "            raw = result.stdout.strip()\n",
    "            json_start = raw.find(\"{\")\n",
    "            json_end = raw.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                llm_result = json.loads(raw[json_start:json_end])\n",
    "                detected_languages.update(llm_result.get(\"programming_languages\", []))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return {\"programming_languages\": sorted(detected_languages)}\n",
    "\n",
    "# Make sure to set your API key in the environment\n",
    "# export OPENAI_API_KEY=\"sk-...\"\n",
    "def analyze_text_with_gpt(pdf_text, model=\"gpt-5-nano\"):\n",
    "    \"\"\"\n",
    "    Analyze PDF text using ChatGPT Responses API.\n",
    "    Returns a set of detected programming languages.\n",
    "    Automatically skips blocked content.\n",
    "    \"\"\"\n",
    "    detected_languages = set()\n",
    "\n",
    "    # Blocked content check\n",
    "    blocked_indicators = [\n",
    "        \"enable javascript and cookies to continue\",\n",
    "        \"access denied\",\n",
    "        \"not found\",\n",
    "        \"error 404\"\n",
    "    ]\n",
    "    preview_text = pdf_text[:300].replace(\"\\n\", \" \").lower()\n",
    "    if any(b in preview_text for b in blocked_indicators):\n",
    "        print(\"‚ö†Ô∏è Blocked content detected, skipping analysis.\")\n",
    "        return {\"programming_languages\": []}\n",
    "\n",
    "    # GPT-5 request\n",
    "    try:\n",
    "        response = openai.responses.create(\n",
    "            model=model,\n",
    "            input=f\"\"\"\n",
    "            You are a text analysis assistant specialized in identifying programming languages mentioned in academic or technical articles.\n",
    "\n",
    "            Task:\n",
    "            1Ô∏è‚É£ Identify **only actual programming languages** used to write code.\n",
    "            2Ô∏è‚É£ Do **NOT** include frameworks, libraries, standards, formal languages, or platforms.\n",
    "            3Ô∏è‚É£ Ignore any text after a ‚ÄúReferences‚Äù or ‚ÄúBibliography‚Äù section.\n",
    "            4Ô∏è‚É£ Return strictly in JSON:\n",
    "\n",
    "            {{\"programming_languages\": [\"Python\", \"C\", \"Java\"]}}\n",
    "\n",
    "            If none found, return:\n",
    "\n",
    "            {{\"programming_languages\": []}}\n",
    "\n",
    "\n",
    "            Text:\n",
    "            {pdf_text}\n",
    "\"\"\"\n",
    "        )\n",
    "        raw = response.output_text.strip()\n",
    "        json_start = raw.find(\"{\")\n",
    "        json_end = raw.rfind(\"}\") + 1\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            llm_result = json.loads(raw[json_start:json_end])\n",
    "            detected_languages.update(llm_result.get(\"programming_languages\", []))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è GPT analysis failed: {e}\")\n",
    "\n",
    "    return {\"programming_languages\": sorted(detected_languages)}\n",
    "\n",
    "# ----------------------\n",
    "# Main loop\n",
    "# ----------------------\n",
    "def process_all_obras():\n",
    "    obras = read_obras_from_csv(OBRAS_CSV)\n",
    "    print(f\"Found {len(obras)} obras in CSV.\")\n",
    "\n",
    "    next_tecn_id = init_csv(TECN_CSV, headers=[\"id\",\"nombre\"])\n",
    "    next_link_id = init_csv(OBRA_TECN_CSV, headers=[\"id\",\"obra_id\",\"tecnologia_id\"])\n",
    "\n",
    "    for obra_id, pdf_url, doi in obras:\n",
    "        print(f\"\\nüîπ Processing Obra ID: {obra_id}\")\n",
    "        try:\n",
    "            # Step 1: fetch PDF text\n",
    "            text, final_url = get_text_from_pdf_url(pdf_url, doi)\n",
    "            if not text:\n",
    "                print(f\"‚ùå No valid PDF or text found.\")\n",
    "                print(f\"‚ö†Ô∏è Skipping Obra ID {obra_id}, no text extracted.\")\n",
    "                continue\n",
    "            else:\n",
    "                preview = text[:300].replace(\"\\n\", \" \").strip()\n",
    "                print(f\"üìÑ Text extracted for Obra ID {obra_id} ({len(text)} chars)\")\n",
    "                print(f\"üîó Source URL used: {final_url}\")\n",
    "                print(f\"üìù Text preview: {preview}{'...' if len(text) > 300 else ''}\")\n",
    "\n",
    "            # Step 2: analyze with Ollama\n",
    "            print(f\"ü§ñ Analyzing text for Obra ID {obra_id}...\")\n",
    "            try:\n",
    "                result = analyze_text_with_gpt(text)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Analysis failed for Obra ID {obra_id}: {e}\")\n",
    "                result = {\"programming_languages\": []}\n",
    "\n",
    "            languages = result.get(\"programming_languages\", [])\n",
    "            print(f\"üìù Obra ID {obra_id} languages detected: {languages}\")\n",
    "\n",
    "            tech_map = load_tecnologias(TECN_CSV)  # { \"Python\": 89, \"C\": 90, ... }\n",
    "\n",
    "            for lang in languages:\n",
    "                if lang not in tech_map:\n",
    "                    tech_map[lang] = next_tecn_id\n",
    "                    append_to_csv(TECN_CSV, [next_tecn_id, lang], headers=[\"id\",\"nombre\"])\n",
    "                    next_tecn_id += 1\n",
    "\n",
    "                tecnologia_id = tech_map[lang]\n",
    "                append_to_csv(OBRA_TECN_CSV, [next_link_id, obra_id, tecnologia_id], headers=[\"id\",\"obra_id\",\"tecnologia_id\"])\n",
    "                next_link_id += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Unexpected error processing Obra ID {obra_id}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Uncomment if you are testing runs \n",
    "    \"\"\"for csv_file in [TECN_CSV, OBRA_TECN_CSV]:\n",
    "        if os.path.exists(csv_file):\n",
    "            os.remove(csv_file)\n",
    "            print(f\"üóëÔ∏è Deleted old CSV: {csv_file}\")\"\"\"\n",
    "    process_all_obras()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1701b",
   "metadata": {},
   "source": [
    "### ALMACENAMIENTO EN POSTGRESQL\n",
    "---\n",
    "\n",
    "Con los csvs llenos y PostgreSQL (por docker compose) conectado y con la BD creada, lo siguiente es llenarla. Como en el algoritmo de creaci√≥n de la BD, es necesario pasar los par√°metros de la base de datos para realizar la conexi√≥n con PostgreSQL: **host**, **port**, **database**, **user** y **password**. Con esta, ya tenemos el cursosr y podemos empezar con el proceso.\n",
    "\n",
    "La idea aqu√≠ es extraer los datos de los csvs con **Dataframes de pandas** y con estos hacer las distintas consultas para insertar (_INSERT into ..._) los dataframes en la BD. Por √∫ltimo, se realiza el commit y se cierra el cursosr y la conexi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "DB_PARAMS = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"demoDB\",\n",
    "    \"user\": \"userPSQL\",\n",
    "    \"password\": \"passPSQL\"\n",
    "}\n",
    "\n",
    "def main():\n",
    "    connection = psycopg2.connect(**DB_PARAMS)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    dir_cache = os.path.join(script_dir, '../cache')\n",
    "\n",
    "    # File paths\n",
    "    file_tematica = os.path.join(dir_cache, 'tematica.csv')\n",
    "    file_tematica_contenida = os.path.join(dir_cache, 'tematica_contenida.csv')\n",
    "    file_obra = os.path.join(dir_cache, 'obra.csv')\n",
    "    file_tecnologia = os.path.join(dir_cache, 'tecnologia.csv')\n",
    "    file_obra_tecnologia = os.path.join(dir_cache, 'obra_tecnologia.csv')\n",
    "\n",
    "    # Read CSVs\n",
    "    df_tematica = pd.read_csv(file_tematica)\n",
    "    df_tematica_contenida = pd.read_csv(file_tematica_contenida)\n",
    "    df_obra = pd.read_csv(file_obra)\n",
    "    df_tecnologia = pd.read_csv(file_tecnologia)\n",
    "    df_obra_tecnologia = pd.read_csv(file_obra_tecnologia)\n",
    "\n",
    "    # Insert tematica\n",
    "    for _, row in df_tematica.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tematica (id, nombre_campo)\n",
    "            VALUES (%s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (int(row['id']), row['nombre_campo'].strip() if pd.notna(row['nombre_campo']) else None))\n",
    "\n",
    "    # Insert tematica_contenida\n",
    "    for _, row in df_tematica_contenida.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tematica_contenida (id, tematica_padre_id, tematica_hijo_id)\n",
    "            VALUES (%s, %s, %s)\n",
    "            ON CONFLICT DO NOTHING\n",
    "        \"\"\", (int(row['id']), int(row['id_padre']), int(row['id_hijo'])))\n",
    "\n",
    "    # Insert obra\n",
    "    for _, row in df_obra.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO obra (\n",
    "                id, doi, direccion_fuente, titulo, abstract, fecha_publicacion,\n",
    "                idioma, num_citas, fwci, tematica_id\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (\n",
    "            int(row['id']),\n",
    "            row.get('doi').strip() if pd.notna(row.get('doi')) else None,\n",
    "            row['direccion_fuente'].strip() if pd.notna(row.get('direccion_fuente')) else None,\n",
    "            row['titulo'].strip() if pd.notna(row.get('titulo')) else None,\n",
    "            row.get('abstract').strip() if pd.notna(row.get('abstract')) else None,\n",
    "            row.get('fecha_publicacion') if pd.notna(row.get('fecha_publicacion')) else None,\n",
    "            row.get('idioma').strip() if pd.notna(row.get('idioma')) else None,\n",
    "            int(row.get('num_citas', 0)) if pd.notna(row.get('num_citas')) else 0,\n",
    "            float(row.get('fwci', 0.0)) if pd.notna(row.get('fwci')) else 0.0,\n",
    "            int(row.get('tematica_id')) if pd.notna(row.get('tematica_id')) else None\n",
    "        ))\n",
    "\n",
    "    # Insert tecnologia\n",
    "    for _, row in df_tecnologia.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tecnologia (id, nombre, tipo, version)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (\n",
    "            int(row['id']),\n",
    "            row['nombre'].strip() if pd.notna(row['nombre']) else None,\n",
    "            row.get('tipo').strip() if pd.notna(row.get('tipo')) else None,\n",
    "            row.get('version').strip() if pd.notna(row.get('version')) else None\n",
    "        ))\n",
    "\n",
    "    # Insert obra_tecnologia\n",
    "    for _, row in df_obra_tecnologia.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO obra_tecnologia (id, obra_id, tecnologia_id)\n",
    "            VALUES (%s, %s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (int(row['id']), int(row['obra_id']), int(row['tecnologia_id'])))\n",
    "# Puede que on conflict falle por lo de id \n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"‚úÖ CSV data loaded successfully including tecnologia and obra_tecnologia.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dadfffb",
   "metadata": {},
   "source": [
    "### CREACI√ìN DEL GRAFO (TURTLE, SCHEMA & SKOS)\n",
    "---\n",
    "Para la importaci√≥n de datos a GraphDB hemos seguido la l√≥gica de utilizar  _Schema_ y _SKOS_ como indicamos en el apartado inicial. A esto se le suma la librer√≠a _Graph_ de _Python_, que es la encargada de crear el grafo del sistema. Para la conexi√≥n de la BD, se sigue utilizando la l√≥gica de conexi√≥n con cursor. Como la base de datos est√° conectada, se utilizan los datos directamente ah√≠ para crear el archivo _Turtle_.\n",
    "\n",
    "Para cada tabla, se selecciona su conjunto de datos completo y a partir de aqu√≠ el proceso cmabia si es una entidad o una relaci√≥n:\n",
    "\n",
    "* **Entidad:** Aqu√≠ primero se identifica a cada objeto con un identificador con la estrutura *tabla_uri*. Este es el nombre del nodo correspondiente al objeto. Posteriormente, con SKOS o Schema (seg√∫n la tabla), se a√±ade el resto de datos al nodo.\n",
    "* **Relaci√≥n:** Aqu√≠ la etiqueta *tabla_uri* solamente es necesaria para *tematica_contenida* ya que es una relaci√≥n entre una tabla consigo misma. Para representar esta relaci√≥n fue necesario el uso de _SKOS_ ya que sus funciones _narrower_ y _broader_ representan la relaci√≥n padre-hijo e hijo-padre respectivamente. Para *obra_tecnologia* por ser entre distintas tablas, solo hac√≠a falta reprsentar la relaci√≥n con _Schema_.\n",
    "\n",
    "Con el grafo terminado, se serializa y se almacena en la carpeta **ttl** como *openalex_graph.ttl*. Con este archivo solo har√≠a falta subirlo al servidor de GraphDB y ya all√≠ hacer las consultas en _SPARQL_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004db1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import psycopg2\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, SKOS, XSD\n",
    "\n",
    "# --- Namespaces ---\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "OPENALEX = Namespace(\"https://openalex.org/\")\n",
    "g = Graph()\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"skos\", SKOS)\n",
    "g.bind(\"openalex\", OPENALEX)\n",
    "\n",
    "# --- PostgreSQL connection ---\n",
    "DB_PARAMS = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"demoDB\",\n",
    "    \"user\": \"userPSQL\",\n",
    "    \"password\": \"passPSQL\"\n",
    "}\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(**DB_PARAMS)\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Connected to database ‚úÖ\")\n",
    "\n",
    "# --- 1. TEM√ÅTICA ---\n",
    "cur.execute(\"SELECT id, nombre_campo FROM tematica;\")\n",
    "for tmid, nombre in cur.fetchall():\n",
    "    tema_uri = OPENALEX[f\"tematica_{tmid}\"]\n",
    "    g.add((tema_uri, RDF.type, SKOS.Concept))\n",
    "    g.add((tema_uri, SKOS.prefLabel, Literal(nombre)))\n",
    "\n",
    "print(\"Mapped table: tematica ‚úÖ\")\n",
    "\n",
    "# tematica_contenida ‚Üí skos:broader\n",
    "cur.execute(\"SELECT id, tematica_padre_id, tematica_hijo_id FROM tematica_contenida;\")\n",
    "for tcid, parent, child in cur.fetchall():\n",
    "    parent_uri = OPENALEX[f\"tematica_{parent}\"]\n",
    "    child_uri = OPENALEX[f\"tematica_{child}\"]\n",
    "    g.add((parent_uri, SKOS.narrower, child_uri))\n",
    "    g.add((child_uri, SKOS.broader, parent_uri))\n",
    "\n",
    "# --- 2. TECNOLOG√çA ---\n",
    "cur.execute(\"SELECT id, nombre, tipo, version FROM tecnologia;\")\n",
    "for tid, nombre, tipo, version in cur.fetchall():\n",
    "    tech_uri = OPENALEX[f\"tecnologia_{tid}\"]\n",
    "    g.add((tech_uri, RDF.type, SCHEMA.SoftwareApplication))\n",
    "    g.add((tech_uri, SCHEMA.name, Literal(nombre)))\n",
    "    if tipo:\n",
    "        g.add((tech_uri, SCHEMA.applicationCategory, Literal(tipo)))\n",
    "    if version:\n",
    "        g.add((tech_uri, SCHEMA.softwareVersion, Literal(version)))\n",
    "\n",
    "print(\"Mapped table: tecnologia ‚úÖ\")\n",
    "\n",
    "# --- 3. OBRA ---\n",
    "cur.execute(\"SELECT id, doi, direccion_fuente, titulo, abstract, fecha_publicacion, idioma, num_citas, fwci, tematica_id FROM obra;\")\n",
    "for oid, doi, direccion_fuente, titulo, abstract, fecha_publicacion, idioma, num_citas, fwci, tematica_id in cur.fetchall():\n",
    "    obra_uri = OPENALEX[f\"obra_{oid}\"]\n",
    "    g.add((obra_uri, RDF.type, SCHEMA.TechArticle))\n",
    "    if doi:\n",
    "        g.add((obra_uri, SCHEMA.sameAs, Literal(doi)))\n",
    "    if direccion_fuente:\n",
    "        g.add((obra_uri, SCHEMA.url, Literal(direccion_fuente)))\n",
    "    if titulo:\n",
    "        g.add((obra_uri, SCHEMA.name, Literal(titulo)))\n",
    "    if abstract:\n",
    "        g.add((obra_uri, SCHEMA.abstract, Literal(abstract)))\n",
    "    if fecha_publicacion:\n",
    "        g.add((obra_uri, SCHEMA.datePublished, Literal(fecha_publicacion, datatype=XSD.date)))\n",
    "    if idioma:\n",
    "        g.add((obra_uri, SCHEMA.inLanguage, Literal(idioma)))\n",
    "    if num_citas:\n",
    "        g.add((obra_uri, SCHEMA.citationCount, Literal(num_citas, datatype=XSD.integer)))\n",
    "    if fwci:\n",
    "        g.add((obra_uri, SCHEMA.metric, Literal(fwci, datatype=XSD.float)))\n",
    "    if tematica_id:\n",
    "        g.add((obra_uri, SCHEMA.about, OPENALEX[f\"tematica_{tematica_id}\"]))\n",
    "\n",
    "print(\"Mapped table: obra ‚úÖ\")\n",
    "\n",
    "# obra_tecnologia ‚Üí schema:mentions\n",
    "cur.execute(\"SELECT obra_id, tecnologia_id FROM obra_tecnologia;\")\n",
    "for oid, tid in cur.fetchall():\n",
    "    g.add((OPENALEX[f\"obra_{oid}\"], SCHEMA.mentions, OPENALEX[f\"tecnologia_{tid}\"]))\n",
    "\n",
    "\n",
    "print(\"Mapped relationships ‚úÖ\")\n",
    "\n",
    "# --- 4. EXPORT ---\n",
    "output_file = \"ttl/openalex_graph.ttl\"\n",
    "g.serialize(destination=output_file, format=\"turtle\")\n",
    "print(f\"RDF graph exported to {output_file} üß©\")\n",
    "\n",
    "# --- 5. Cleanup ---\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"PostgreSQL connection closed üîí\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ebbb30",
   "metadata": {},
   "source": [
    "### CREACI√ìN DE CONSULTAS\n",
    "---\n",
    "\n",
    "Para finalizar el proyecto, hemos creado una serie de querys con las que se pueda analizar s√≠ el sistema cumple los objetivos programados.\n",
    "\n",
    "Primeramente, se prob√≥ si cumpl√≠a el objetivo principal: obtener el n√∫mero de apariciones de distintos lenguajes de programaci√≥n en obras de cada subtem√°tica (o tem√°tica sin hijas) distinta. El resultado fue una tabla donde se listaban todas las combinaciones de t√≥pico y lenguaje distinto dentro de la base de datos, junto al n√∫mero de veces que se repet√≠a esa relaci√≥n. Por lo tanto, se cumpli√≥ la meta descrita al inicio del proyecto.\n",
    "\n",
    "Para demostrar su funcionamiento, solo hay que utilizar la siguiente consulta en GraphDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PREFIX schema: <https://schema.org/>\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "\n",
    "# Queremos una lista de \"aristas\" (conexiones)\n",
    "SELECT ?topicName ?techName (COUNT(DISTINCT ?work) AS ?sharedWorksCount)\n",
    "WHERE {\n",
    "    ?work schema:about ?topic .\n",
    "    ?work schema:mentions ?technology .\n",
    "    \n",
    "    OPTIONAL { ?topic skos:prefLabel ?topicName . }\n",
    "    OPTIONAL { ?technology schema:name ?techName . }\n",
    "    \n",
    "    FILTER(BOUND(?topicName) && BOUND(?techName))\n",
    "}\n",
    "GROUP BY ?topicName ?techName\n",
    "ORDER BY DESC(?sharedWorksCount)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8cb0d8",
   "metadata": {},
   "source": [
    "Posteriormente, para exprimir los l√≠mites del sistema, decidimos probar una consulta m√°s: Tecnolog√≠as m√°s competitivas entre campos. En esta consulta, se lista el n√∫mero de veces que cada par de tecnolog√≠as distintas (por ejemplo, Python y Java, C++ y Prolog, etc.) aparecen en las mismas obras. Con el resultado, aprendimos que Python y Java eran la dupla que m√°s aparec√≠a en las mismas obras.\n",
    "\n",
    "Para demostrar su funcionamiento, solo hay que utilizar la siguiente consulta en GraphDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PREFIX schema: <https://schema.org/>\n",
    "\n",
    "SELECT ?techA_name ?techB_name (COUNT(DISTINCT ?topic) AS ?commonTopics)\n",
    "WHERE {\n",
    "    # Encuentra la primera tecnolog√≠a (A) mencionada por una obra\n",
    "    ?workA schema:mentions ?techA .\n",
    "    ?workA schema:about ?topic .\n",
    "    ?techA schema:name ?techA_name .\n",
    "\n",
    "    # Encuentra la segunda tecnolog√≠a (B) mencionada por la misma obra (o una obra sobre el mismo tema)\n",
    "    ?workB schema:mentions ?techB .\n",
    "    ?workB schema:about ?topic . # <-- Mismo tema\n",
    "    ?techB schema:name ?techB_name .\n",
    "\n",
    "    # Asegura que no sea la misma tecnolog√≠a\n",
    "    FILTER (?techA != ?techB)\n",
    "\n",
    "    # Solo queremos los nombres, no las URIs largas\n",
    "    FILTER(BOUND(?techA_name) && BOUND(?techB_name))\n",
    "}\n",
    "GROUP BY ?techA_name ?techB_name\n",
    "ORDER BY DESC(?commonTopics)\n",
    "LIMIT 10\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
