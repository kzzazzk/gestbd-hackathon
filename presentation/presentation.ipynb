{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f82daac",
   "metadata": {},
   "source": [
    "![Open Alex Logo](./images/openalex-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac50be",
   "metadata": {},
   "source": [
    "## PLANTEAMIENTO Y ACOTACI√ìN DE OBJETIVOS INICIALES\n",
    "El prop√≥sito inicial pretend√≠a responder a la pregunta de qu√© instituciones y pa√≠ses son los mayores divulgadores de conocimientos en cada rama del conocimiento humano.\n",
    "\n",
    "Este objetivo era totalmente cumplible si no fuese porque openalex ya nos devolv√≠a todos los datos de forma estructurada:\n",
    "\n",
    "![Diagram OpenAlex components](./images/openalex-diagram.jpg)\n",
    "\n",
    "Es decir, en este punto del proyecto carec√≠amos de fuentes de datos no estructuradas:\n",
    "\n",
    "* Por ello, optamos por dirigir el proyecto a un enfoque diferente que parte del inter√©s de comprender que tecnolog√≠as de programaci√≥n son las mas usadas, en art√≠culos o proyectos de investigaci√≥n relacionados con la computaci√≥n, y como var√≠a su uso y distribuci√≥n en funci√≥n de cada subcampo.\n",
    "\n",
    "Adem√°s, identificamos una problem√°tica con el alcance definido para el proyecto (dado que se desarrollar√≠a exclusivamente en 5h durante el Hackathon):\n",
    "\n",
    "* OpenAlex cuenta con un total de **200 Millones de archivos**, lo cual implicar√≠a una cantidad alarmante de tiempo para extraer y procesar toda la informaci√≥n necesaria para cumplir el objetivo. Por lo tanto, decidimos utilizar un conjunto de restricciones para acotar el alcance de los datos:\n",
    "\n",
    "    * Usamos el campo _Tem√°tica_ para filtrar el n√∫mero de archivos a extraer, en este aspecto decidimos filtrar una sucesi√≥n de tem√°ticas padre-hijo relacionadas con nuestra meta: \n",
    "\n",
    "        ***Physical Sciences (domain) -> Computer Science (field) -> Artificial Inteligence (subfield) -> Todas las tem√°ticas hijas de AI sin hijo (topics)***\n",
    "\n",
    "    * Agregamos un conjunto de campos restrictivos como que el idioma en el que est√©n escritos sea en ingl√©s, agregamos filtrado de keyword por lenguajes de programaci√≥n en art√≠culos y pedimos exclusivamente los articulos que tengan vinculado su pdf y adem√°s que est√© sea open access. Esto redujo el n√∫mero de obras a procesar a ~6000 art√≠culos:\n",
    "\n",
    "![Filtro tem√°ticas dentro de OpenAlex](./images/filtro_openalex.jpeg)\n",
    "\n",
    "\n",
    "Por √∫ltimo decidimos adem√°s simplificar mucho m√°s las entidades a almacenar, reduciendo lo m√°ximo posible la estructura y manteniendo la funcionalidad que se quer√≠a obtener con el objetivo. A continuaci√≥n se muestra la imagen de la estructura completa de la base de datos antes y despu√©s de modificarla:\n",
    "\n",
    "### Diagrama Entidad-Relaci√≥n del sistema antes de la adaptaci√≥n\n",
    "![Diagrama Entidad-Relaci√≥n del sistema antes](./images/relaciones%20db.png)\n",
    "### Diagrama Entidad-Relaci√≥n del sistema despu√©s de la adaptaci√≥n\n",
    "![Diagrama Entidad-Relaci√≥n del sistema despu√©s](./images/esquema_redux.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd83b5f",
   "metadata": {},
   "source": [
    "## TRABAJO REALIZADO EN EL HACKATHON\n",
    "\n",
    "### CREACI√ìN BASE DE DATOS POSTGRESQL\n",
    "---\n",
    "\n",
    "\n",
    "En nuestra base de datos de PostgreSQL representamos las entidades como las siguientes tablas:\n",
    "\n",
    "* **Obra:** Engloba los art√≠culos citados dentro de OpenAlex. Tiene dos funciones esenciales:\n",
    "    * Almacenar los datos estructurados de cada art√≠culo organizados por sus atributos: nombre, doi, url, etc.\n",
    "    * Enlazar  _Tecnologia_ y _Tematica_ para poder realizar b√∫squedas de datos filtrando por ambas entidades. Cabe destacar que, como _Tecnologia_ y _Obra_ tienen una relaci√≥n N:M, fue necesario crear una tabla para representar esta relaci√≥n. Esto no pasa con la relaci√≥n entre _Obra_ y _Tematica_ ya que, como es 1:N, con indicar un atributo *tematica_id* es suficiente.\n",
    "\n",
    "* **Tecnologia:** Engloba los distintos lenguajes de programaci√≥n obtenidos al extraer y analizar el contenido de los art√≠culos. Los datos son solamente no estructurados. Est√° enlazado con _Obra_ por *Obra_tecnologia*.\n",
    "\n",
    "* **Tematica:** Aqu√≠ se alamacena cada tem√°tica encontrada en la b√∫squeda de OpenAlex, por lo que los datos de esta tabla son estructurados. Se encuentra relacionado con _Obra_ gracias a que las obras de una tem√°tica concreta almacenan su id. Lo importante aqu√≠ es que existen tem√°ticas que son subtem√°ticas de otras. La idea es que las obras est√©n relacionadas solamente con tem√°ticas sin hijos para que estas a su vez se relacionen con sus padres, haciendo que todas las obras de cada hijo se relacionen con el padre. Para hacer esta relaci√≥n de _Tematica_ con _Tematica_, fue necesaria la creaci√≥n de una nueva tabla: *tematica_contenida*.\n",
    "\n",
    "Adem√°s, se han creado estas tablas para las relaciones:\n",
    "\n",
    "* **Obra_tecnologia:** Hace referencia a la relaci√≥n N:M entre _Tecnologia_ y _Obra_.\n",
    "\n",
    "* **Tematica_contenida:** Hace referencia a la relaci√≥n padre a hijo de _Tematica_ con _Tematica_.\n",
    "\n",
    "Todas estas tablas fueron creadas dentro de un *SQL script*. Para subirla a PostgreSQL, se conecta el docker, se indican los par√°metros necesarios en el Python y, si no hay ning√∫n problema, se suben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595cfe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Database 'demoDB' already exists.\n",
      "‚úÖ Tables created or verified successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "DB_PARAMS = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"demoDB\",\n",
    "    \"user\": \"userPSQL\",\n",
    "    \"password\": \"passPSQL\"\n",
    "}\n",
    "\n",
    "sql_script = \"\"\"CREATE TABLE IF NOT EXISTS tematica (\n",
    "    id INTEGER,\n",
    "    nombre_campo TEXT NOT NULL,\n",
    "        PRIMARY KEY (id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS tecnologia (\n",
    "    id INTEGER,\n",
    "    nombre TEXT NOT NULL,\n",
    "    tipo TEXT,\n",
    "    version TEXT,\n",
    "    PRIMARY KEY (id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS obra (\n",
    "    id INTEGER,\n",
    "    doi TEXT UNIQUE,  -- NEW\n",
    "    direccion_fuente TEXT NOT NULL,\n",
    "    titulo TEXT NOT NULL,\n",
    "    abstract TEXT,\n",
    "    fecha_publicacion TEXT,\n",
    "    idioma TEXT,\n",
    "    num_citas INTEGER DEFAULT 0,\n",
    "    fwci REAL,\n",
    "    tematica_id INTEGER,\n",
    "    PRIMARY KEY (id),\n",
    "    FOREIGN KEY (tematica_id)\n",
    "        REFERENCES tematica(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE SET NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS obra_tecnologia (\n",
    "    id INTEGER,\n",
    "    obra_id INTEGER NOT NULL,\n",
    "    tecnologia_id INTEGER NOT NULL,\n",
    "    PRIMARY KEY (id),\n",
    "    FOREIGN KEY (obra_id)\n",
    "        REFERENCES obra(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE CASCADE,\n",
    "    FOREIGN KEY (tecnologia_id)\n",
    "        REFERENCES tecnologia(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE RESTRICT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS tematica_contenida (\n",
    "    id INTEGER,\n",
    "    tematica_padre_id INTEGER NOT NULL,\n",
    "    tematica_hijo_id INTEGER NOT NULL,\n",
    "    PRIMARY KEY (id),\n",
    "    FOREIGN KEY (tematica_padre_id)\n",
    "        REFERENCES tematica(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE CASCADE,\n",
    "    FOREIGN KEY (tematica_hijo_id)\n",
    "        REFERENCES tematica(id)\n",
    "        ON UPDATE CASCADE\n",
    "        ON DELETE CASCADE,\n",
    "    CHECK (tematica_padre_id <> tematica_hijo_id)\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_obra_tematica ON obra(tematica_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_obratec_tecnologia ON obra_tecnologia(tecnologia_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_tematica_hijo ON tematica_contenida(tematica_hijo_id);\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    # Connect to default database to check/create demoDB\n",
    "    connection = psycopg2.connect(\n",
    "        host=DB_PARAMS['host'],\n",
    "        port=DB_PARAMS['port'],\n",
    "        database=\"demoDB\",\n",
    "        user=DB_PARAMS['user'],\n",
    "        password=DB_PARAMS['password']\n",
    "    )\n",
    "    connection.autocommit = True\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT 1 FROM pg_database WHERE datname = %s;\", (DB_PARAMS[\"database\"],))\n",
    "    exists = cursor.fetchone()\n",
    "\n",
    "    if not exists:\n",
    "        cursor.execute(sql.SQL(f\"CREATE DATABASE {DB_PARAMS['database']};\"))\n",
    "        print(f\"‚úÖ Database '{DB_PARAMS['database']}' created.\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Database '{DB_PARAMS['database']}' already exists.\")\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    # Connect to demoDB to create tables\n",
    "    connection = psycopg2.connect(**DB_PARAMS)\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(sql_script)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"‚úÖ Tables created or verified successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962917fe",
   "metadata": {},
   "source": [
    "### OBTENCI√ìN DATOS ESTRUCTURADOS (REQUESTS & CSV)\n",
    "---\n",
    "\n",
    "La idea ahora es obtener los datos estructurados, es decir, el contenido de _Obra_, _Tematica_ y *Tematica_contenida*. Para utilizar posteriormente estos datos, se ha creado una carpeta **cache** para almacenar estas tablas ya con la informaci√≥n estructurada en formato csv. Para explicar el proceso completo de esta fase, podemos dividirla en las siguientes subfases:\n",
    "\n",
    "1. Creaci√≥n del csv de la entidad _obra_\n",
    "2. B√∫squeda y obtenci√≥n de los art√≠culos pedidos.\n",
    "3. Creaci√≥n del csv de la entidad _tematica_\n",
    "4. Creaci√≥n del csv de la relaci√≥n *tematica_contenida*\n",
    "\n",
    "En el c√≥digo, se puede observar que el main sigue una estructura equivalente:\n",
    "\n",
    "```\n",
    "def main():\n",
    "    initialize_csv_files()\n",
    "    tematica_map = fetch_all_works()\n",
    "    save_tematica_csv(tematica_map)\n",
    "    update_tematica_and_generate_tematica_contenida()\n",
    "```\n",
    "\n",
    "Adem√°s, cabe destacar que se han declarado variables al inicio del c√≥digo para que la b√∫squeda siga los filtros explicados en la introducci√≥n y para tener ya almacenadas las direcciones donde se crearan los csvs:\n",
    "\n",
    "```\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "PER_PAGE = 200\n",
    "KEYWORDS = [\n",
    "    \"python\",\"c-programming-language\",\"javascript\",\"java\",\"java-programming-language\",\n",
    "    \"sql\",\"dart\",\"swift\",\"cobol\",\"fortran\",\"matlab\",\"prolog\",\"lisp\",\"haskell\",\"rust\",\"perl\",\n",
    "    \"scala\",\"html\",\"html5\"\n",
    "]\n",
    "SUBFIELD_ID = \"subfields/1702\"\n",
    "LANGUAGE = \"languages/en\"\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # go up one level from src/\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
    "\n",
    "CSV_OBRA = os.path.join(CACHE_DIR, \"obra.csv\")\n",
    "CSV_TEMATICA = os.path.join(CACHE_DIR, \"tematica.csv\")\n",
    "CSV_TEMATICA_CONTENIDA = os.path.join(CACHE_DIR, \"tematica_contenida.csv\")\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "BASE_TOPICS = [\"Physical Sciences\", \"Computer Science\", \"Artificial Intelligence\"]\n",
    "```\n",
    "#### Creaci√≥n del csv de la entidad _obra_\n",
    "Crea el archivo *obra.csv* (o lo abre si ya esta creado) y crea la tabla vac√≠a (atributos son las columnas y habr√° tantas filas como obras distintas). Este proceso se encuentra represnetado dentro de la funci√≥n *initialize_csv_files*.\n",
    "\n",
    "#### B√∫squeda y obtenci√≥n de los art√≠culos pedidos.\n",
    "Como no todas las obras aparecen por p√°gina (l√≠mite: **200**). Este proceso se ha creado siguiendo esta l√≥gica:\n",
    "1. La b√∫squeda se va a realizar en bucle, por lo que es necesario crear los valores no afectados antes de empezar. Las que tienen mayor importancia son la listado/diccionario de tem√°ticas (*tematic_map*), el id de la tem√°tica siguiente (*next_tematica_id*) y la p√°gina actual (*page*).\n",
    "\n",
    "2. Empieza al bucle. Al inicio se realiza la llamada a OpenAlex. A esta se le pasa por par√°metros la p√°gina actual (*page*), el l√≠mite de 200 (*PER_PAGE*), un filtro formado por los datos declarados previamente (*filter*) y el orden (*sort*). Solo en caso deque la funci√≥n que realiza la llamada (*fetch_page*) devuelva los resultados se sigue con el proceso.\n",
    "\n",
    "3. Saca todos los datos estrcuturados para las tres tablas, usando el identificador correspondiente de cada atributo dentro de OpenAlex. El abstract se restructura para que se pueda insertar en el csv (*reconstruct_abstract*)\n",
    "\n",
    "4. En cuanto a la tem√°tica. Se obtiene su nombre. Si ya est√° en *tematic_map* se ignora. Si no, se a√±ade al diccionario, siendo el nombre la clave y su valor el *next_tematica_id*, el cual es su nuevo id. Al valor de esta √∫ltima variable se le suma uno para la siguiente ejecuci√≥n.\n",
    "\n",
    "5. Se saca *tematica_id* con *tematic_map* y se crea una nueva fila en *obra_csv* con los datos del art√≠culo. Si el bucle ha terminado con los art√≠culos de la p√°gina, va a la siguiente (*page* + 1) y repite el bucle. Si ya no se encuentran m√°s obras, se finaliza.\n",
    "\n",
    "6. Se devuelve *tematic_map* para la creaci√≥n de los otros dos csv.\n",
    "\n",
    "La funci√≥n que sigue este proceso es *fetch_all_works*.\n",
    "\n",
    "#### Creaci√≥n del csv de la entidad _tematica_\n",
    "Se crea el *tematica_csv* tal cual se cre√≥ el de _obra_, pero como se tienen ya el n√∫mero de tem√°ticas con sus ids gracias a *tematic_map*, se llena la tabla al completo con todas las tem√°ticas. La funci√≥n que sigue este proceso es *save_tematica_csv*.\n",
    "\n",
    "#### Creaci√≥n del csv de la relaci√≥n *tematica_contenida*\n",
    "Esta funci√≥n se ha creado con la idea de que ya sabemos quienes son los padres: ***Physical Sciences -> Computer_Science -> Artificial Inteligence -> Tem√°ticas sin hijo***. Por tanto, a partir de cada tem√°tica dentro de *tematica_csv*, se analiza si es una de los posibles padres. Si es una sin hijos, se declara como hija de *Artificial Inteligence* con un identificador propio (m√°s el de *Artificial Inteligence* y el hijo). Para las que se son padres, se decalra directamente la relaci√≥n a partir de la estructura antes mencionada. Con esto, se crea *tematica_contenida.csv*  y en este archivo se primero los nombres de las columnas (*id*, *id_padre* e *id_hijo*) y posteriormente tantas filas como relaciones detectadas (ids distintos), incluidas las relaciones entre los padres. La funci√≥n que sigue este proceso es *update_tematica_and_generate_tematica_contenida*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e722b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenAlex fetch process...\n",
      "Initialized 'c:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\cache\\obra.csv' for writing works.\n",
      "Total results to fetch (approximate): 6067\n",
      "Fetched page 1 with 200 works.\n",
      "Fetched page 2 with 200 works.\n",
      "Fetched page 3 with 200 works.\n",
      "Fetched page 4 with 200 works.\n",
      "Fetched page 5 with 200 works.\n",
      "Fetched page 6 with 200 works.\n",
      "Fetched page 7 with 200 works.\n",
      "Fetched page 8 with 200 works.\n",
      "Fetched page 9 with 200 works.\n",
      "Fetched page 10 with 200 works.\n",
      "Fetched page 11 with 200 works.\n",
      "Fetched page 12 with 200 works.\n",
      "Fetched page 13 with 200 works.\n",
      "Fetched page 14 with 200 works.\n",
      "Fetched page 15 with 200 works.\n",
      "Fetched page 16 with 200 works.\n",
      "Fetched page 17 with 200 works.\n",
      "Fetched page 18 with 200 works.\n",
      "Fetched page 19 with 200 works.\n",
      "Fetched page 20 with 200 works.\n",
      "Fetched page 21 with 200 works.\n",
      "Fetched page 22 with 200 works.\n",
      "Fetched page 23 with 200 works.\n",
      "Fetched page 24 with 200 works.\n",
      "Fetched page 25 with 200 works.\n",
      "Fetched page 26 with 200 works.\n",
      "Fetched page 27 with 200 works.\n",
      "Fetched page 28 with 200 works.\n",
      "Fetched page 29 with 200 works.\n",
      "Fetched page 30 with 200 works.\n",
      "Fetched page 31 with 67 works.\n",
      "Finished fetching all works. Total works saved: 6067\n",
      "Saved 'c:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\cache\\tematica.csv' with 73 topics.\n",
      "Added base topic 'Physical Sciences' with id=74\n",
      "Added base topic 'Computer Science' with id=75\n",
      "Added base topic 'Artificial Intelligence' with id=76\n",
      "'c:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\cache\\tematica.csv' updated with 76 topics.\n",
      "'c:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\cache\\tematica_contenida.csv' generated with 75 relations.\n",
      "Finished fetching and processing all works and topics.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "PER_PAGE = 200\n",
    "\n",
    "KEYWORDS = [\n",
    "    \"python\",\"c-programming-language\",\"javascript\",\"java\",\"java-programming-language\",\n",
    "    \"sql\",\"dart\",\"swift\",\"cobol\",\"fortran\",\"matlab\",\"prolog\",\"lisp\",\"haskell\",\"rust\",\"perl\",\n",
    "    \"scala\",\"html\",\"html5\"\n",
    "]\n",
    "SUBFIELD_ID = \"subfields/1702\" #Artificial Intelligence\n",
    "LANGUAGE = \"languages/en\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = str(NOTEBOOK_DIR.parent)\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
    "\n",
    "CSV_OBRA = os.path.join(CACHE_DIR, \"obra.csv\")\n",
    "CSV_TEMATICA = os.path.join(CACHE_DIR, \"tematica.csv\")\n",
    "CSV_TEMATICA_CONTENIDA = os.path.join(CACHE_DIR, \"tematica_contenida.csv\")\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "BASE_TOPICS = [\"Physical Sciences\", \"Computer Science\", \"Artificial Intelligence\"] # Base topics to ensure presence of hierarchy\n",
    "\n",
    "def reconstruct_abstract(abstract_inverted_index):\n",
    "    if not abstract_inverted_index or not isinstance(abstract_inverted_index, dict):\n",
    "        return \"\"\n",
    "    position_map = {}\n",
    "    for word, positions in abstract_inverted_index.items():\n",
    "        for pos in positions:\n",
    "            position_map[pos] = word\n",
    "    return \" \".join(position_map[pos] for pos in sorted(position_map.keys()))\n",
    "\n",
    "def fetch_page(url, params, max_retries=5, delay_base=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            print(f\"‚ö†Ô∏è Warning: Bad response {response.status_code}, retrying...\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Exception during request: {e}\")\n",
    "        retries += 1\n",
    "        time.sleep(delay_base ** retries)\n",
    "    print(f\"‚ùå Error: Failed to fetch page after {max_retries} retries.\")\n",
    "    return None\n",
    "\n",
    "def initialize_csv_files():\n",
    "    os.makedirs(os.path.dirname(CSV_OBRA), exist_ok=True)\n",
    "    with open(CSV_OBRA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"id\",\"direccion_fuente\",\"titulo\",\"abstract\",\"fecha_publicacion\",\n",
    "            \"idioma\",\"num_citas\",\"fwci\",\"tematica_id\",\"doi\"\n",
    "        ])\n",
    "    print(f\"Initialized '{CSV_OBRA}' for writing works.\")\n",
    "\n",
    "def fetch_all_works():\n",
    "    tematica_map = {}\n",
    "    next_tematica_id = 1\n",
    "    obra_id = 1\n",
    "    page = 1\n",
    "    total_results = None\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"per_page\": PER_PAGE,\n",
    "            \"filter\": f\"open_access.is_oa:true,has_content.pdf:true,primary_topic.subfield.id:{SUBFIELD_ID},best_oa_location.is_accepted:true,language:{LANGUAGE},keywords.id:{'|'.join(KEYWORDS)}\",\n",
    "            \"sort\": \"cited_by_count:desc\"\n",
    "        }\n",
    "        data = fetch_page(BASE_URL, params)\n",
    "        if not data or \"results\" not in data:\n",
    "            print(f\"No data returned for page {page}, stopping.\")\n",
    "            break\n",
    "\n",
    "        works = data[\"results\"]\n",
    "        if total_results is None:\n",
    "            total_results = data.get(\"meta\", {}).get(\"count\", 0)\n",
    "            print(f\"Total results to fetch (approximate): {total_results}\")\n",
    "\n",
    "        print(f\"Fetched page {page} with {len(works)} works.\")\n",
    "\n",
    "        with open(CSV_OBRA, \"a\", newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for work in works:\n",
    "                # --- PDF URL ---\n",
    "                pdf_url = work.get(\"best_oa_location\", {}).get(\"pdf_url\")\n",
    "                if not pdf_url:\n",
    "                    continue\n",
    "\n",
    "                # --- DOI ---\n",
    "                doi = work.get(\"doi\", \"\")\n",
    "\n",
    "                titulo = work.get(\"title\", \"\")\n",
    "                abstract = reconstruct_abstract(work.get(\"abstract_inverted_index\"))\n",
    "                fecha_publicacion = work.get(\"publication_date\", \"\")\n",
    "                idioma = work.get(\"language\", LANGUAGE)\n",
    "                num_citas = work.get(\"cited_by_count\", 0)\n",
    "                fwci = work.get(\"fwci\", \"\")\n",
    "                primary_topic = work.get(\"primary_topic\")\n",
    "                if not primary_topic:\n",
    "                    continue\n",
    "                topic_name = primary_topic.get(\"display_name\", \"Unknown Topic\")\n",
    "                if topic_name not in tematica_map:\n",
    "                    tematica_map[topic_name] = next_tematica_id\n",
    "                    next_tematica_id += 1\n",
    "                tematica_id = tematica_map[topic_name]\n",
    "\n",
    "                writer.writerow([\n",
    "                    obra_id, pdf_url, titulo, abstract, fecha_publicacion,\n",
    "                    idioma, num_citas, fwci, tematica_id, doi\n",
    "                ])\n",
    "                obra_id += 1\n",
    "\n",
    "        if page * PER_PAGE >= total_results or not works:\n",
    "            break\n",
    "        page += 1\n",
    "\n",
    "    print(f\"Finished fetching all works. Total works saved: {obra_id-1}\")\n",
    "    return tematica_map\n",
    "\n",
    "def save_tematica_csv(tematica_map):\n",
    "    os.makedirs(os.path.dirname(CSV_TEMATICA), exist_ok=True)\n",
    "    with open(CSV_TEMATICA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"nombre_campo\"])\n",
    "        for topic_name, topic_id in tematica_map.items():\n",
    "            writer.writerow([topic_id, topic_name])\n",
    "    print(f\"Saved '{CSV_TEMATICA}' with {len(tematica_map)} topics.\")\n",
    "\n",
    "def update_tematica_and_generate_contenida():\n",
    "    if not os.path.exists(CSV_TEMATICA):\n",
    "        raise FileNotFoundError(f\"{CSV_TEMATICA} not found.\")\n",
    "\n",
    "    tematicas = {}\n",
    "    rows = []\n",
    "    with open(CSV_TEMATICA, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            row[\"id\"] = int(row[\"id\"])\n",
    "            rows.append(row)\n",
    "            tematicas[row[\"nombre_campo\"].strip()] = row[\"id\"]\n",
    "\n",
    "    max_id = max(r[\"id\"] for r in rows)\n",
    "    for topic in BASE_TOPICS:\n",
    "        if topic not in tematicas:\n",
    "            max_id += 1\n",
    "            tematicas[topic] = max_id\n",
    "            rows.append({\"id\": max_id, \"nombre_campo\": topic})\n",
    "            print(f\"Added base topic '{topic}' with id={max_id}\")\n",
    "\n",
    "    with open(CSV_TEMATICA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"id\", \"nombre_campo\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    relaciones = [\n",
    "        {\"id_padre\": tematicas[\"Physical Sciences\"], \"id_hijo\": tematicas[\"Computer Science\"]},\n",
    "        {\"id_padre\": tematicas[\"Computer Science\"], \"id_hijo\": tematicas[\"Artificial Intelligence\"]}\n",
    "    ]\n",
    "    ai_id = tematicas[\"Artificial Intelligence\"]\n",
    "    for nombre, id_ in tematicas.items():\n",
    "        if nombre not in BASE_TOPICS:\n",
    "            relaciones.append({\"id_padre\": ai_id, \"id_hijo\": id_})\n",
    "    relaciones = [{\"id_padre\": p, \"id_hijo\": h} for p, h in {(r[\"id_padre\"], r[\"id_hijo\"]) for r in relaciones}]\n",
    "\n",
    "    os.makedirs(os.path.dirname(CSV_TEMATICA_CONTENIDA), exist_ok=True)\n",
    "    with open(CSV_TEMATICA_CONTENIDA, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"id_padre\", \"id_hijo\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(relaciones)\n",
    "\n",
    "    print(f\"'{CSV_TEMATICA}' updated with {len(rows)} topics.\")\n",
    "    print(f\"'{CSV_TEMATICA_CONTENIDA}' generated with {len(relaciones)} relations.\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting OpenAlex fetch process...\")\n",
    "    initialize_csv_files()\n",
    "    tematica_map = fetch_all_works()\n",
    "    save_tematica_csv(tematica_map)\n",
    "    update_tematica_and_generate_contenida()\n",
    "    print(\"Finished fetching and processing all works and topics.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd7bce",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏èCELDA NO EJECUTABLE PORQUE SE EJECUT√ì Y SE GENERARON LOS CSV PARA GUARDAR SUS RESULTADOS PREVIAMENTE (SE TARDARON M√ÅS DE 6H EN PROCESAR LOS RESULTADOS Y NO HA DADO TIEMPO NI HAY RECURSOS PARA VOLVER A EJECUTAR)\n",
    "\n",
    "### OBTENCI√ìN DATOS NO ESTRUCTURADOS \n",
    "\n",
    "\n",
    "\n",
    "Una vez establecidos los datos estructurados, comenzamos la tarea de extracci√≥n de lenguajes de programaci√≥n usando un LLM. Para la comprensi√≥n de esta fase, la dividir√© en subfases:\n",
    "\n",
    "1.  **Extracci√≥n del texto** de los documentos PDF de los art√≠culos.\n",
    "2.  **Generaci√≥n de lenguajes** de programaci√≥n por art√≠culo, enviando el texto extra√≠do de cada obra a `gpt-5-nano`.\n",
    "3.  **Guardado en cach√©** de este procesamiento en los archivos `tecnologia.csv` y `obra_tecnologia.csv` para no tener que volver a ejecutar el LLM en caso de p√©rdida de datos.\n",
    "\n",
    "\n",
    "#### EXTRACCI√ìN DEL TEXTO DE LOS DOCUMENTOS PDF DE LOS ART√çCULOS\n",
    "Esta tarea parece simple, pero realmente es la m√°s compleja para no perder datos. Veamos c√≥mo est√° estructurado el `obra.csv` para comprenderlo:\n",
    "\n",
    "![Filtro tem√°ticas dentro de OpenAlex](./images/obra-csv-screenshot.png)\n",
    "\n",
    "Como se puede observar, hay dos campos que hacen referencia al URL:\n",
    "* **`doi`**: El Digital Object Identifier, este es el identificador √∫nico del art√≠culo.\n",
    "* **`direccion_fuente`**: La *open access url* m√°s fiable desde el punto de vista de OpenAlex.\n",
    "\n",
    "La `direccion_fuente` va a ser la primera URL a la que consultaremos, ya que asumimos que la consulta a OpenAlex siempre devuelve un archivo PDF de *open access* (dado que la restricci√≥n en la consulta as√≠ lo define).\n",
    "\n",
    "El flujo de extracci√≥n es el siguiente:\n",
    "\n",
    "1.  **Intento con `direccion_fuente`**:\n",
    "    * Si la URL responde con un `Content-Type` de `application/pdf`, extraemos el texto directamente usando la librer√≠a `PyMuPDF` y pasamos al siguiente paso.\n",
    "    * Si la URL responde con `text/html`, significa que es una p√°gina web.\n",
    "\n",
    "2.  **Proceso de *Scraping* (si es HTML)**:\n",
    "    * Se analiza el HTML buscando clases comunes que los visores de PDF web usan para renderizar texto (ej: `[class*=\"textLayer\"]`). Si se encuentra texto, se extrae.\n",
    "    * Si no se encuentra texto, se buscan todos los enlaces `<a>` en la p√°gina que terminen en `.pdf` y se intenta descargar el primer enlace encontrado.\n",
    "\n",
    "3.  **Proceso de *Fallback* (si todo lo anterior falla)**:\n",
    "    * Si la `direccion_fuente` falla o no se encuentra un PDF, se utiliza el `doi` del art√≠culo.\n",
    "    * Se consulta la API de **Unpaywall** (`https://api.unpaywall.org/v2/{doi}`) para encontrar la mejor URL de PDF de acceso abierto (`best_oa_location`).\n",
    "    * Paralelamente, se intenta construir una URL directa al portal de **ACM** (`https://dl.acm.org/doi/pdf/{doi}`), que es una fuente com√∫n.\n",
    "\n",
    "4.  **Extracci√≥n Final**:\n",
    "    * Si cualquiera de estas estrategias devuelve un archivo PDF v√°lido, se extrae su contenido con `PyMuPDF`. Si ninguna lo logra, se marca el art√≠culo como fallido y se contin√∫a con el siguiente.\n",
    "\n",
    "#### MANEJO DE ERRORES (HTTP Y \"SOFT 404\")\n",
    "El flujo anterior est√° protegido contra errores de varias maneras para asegurar su robustez:\n",
    "\n",
    "* **Errores HTTP (Timeouts y Conexi√≥n):** Toda la l√≥gica de descarga (`requests.get`) est√° envuelta en un bloque `try...except` gen√©rico. Esto captura errores de red, DNS, o si el servidor tarda demasiado en responder (definido en `PDF_TIMEOUT = 30`). Si ocurre una de estas excepciones, se registra el error y se activa inmediatamente el proceso de *fallback* (Unpaywall/ACM).\n",
    "\n",
    "* **Errores \"Soft 404\" en HTML:** Este es un caso cr√≠tico. A veces, un servidor devuelve un c√≥digo `200 OK` (√©xito) pero la p√°gina HTML es en realidad un error (\"No encontrado\", \"Acceso denegado\", etc.).\n",
    "    * Para detectar esto, si la respuesta es `text/html`, el script extrae el texto plano de la p√°gina (`soup.get_text()`) y lo busca contra una lista de indicadores de error (ej: `[\"not found\", \"error 404\", \"no encontrado\", \"access denied\"]`).\n",
    "    * Si encuentra alguna de estas frases, considera la p√°gina como un error, la descarta, y activa el *fallback* a Unpaywall y ACM.\n",
    "\n",
    "* **Detecci√≥n de Contenido Bloqueado (Pre-LLM):** Existe un √∫ltimo filtro. Incluso si se extrae texto con √©xito, este podr√≠a ser in√∫til (ej. un *paywall* o un aviso de \"habilitar cookies\"). La funci√≥n `analyze_text_with_gpt` revisa los primeros 300 caracteres del texto. Si detecta frases como `[\"enable javascript and cookies to continue\", \"access denied\", ...]`, omite el an√°lisis del LLM para ese art√≠culo, evitando procesar texto basura y gastar recursos.\n",
    "\n",
    "\n",
    "#### GENERACI√ìN DE LENGUAJES V√çA LLM\n",
    "Una vez obtenido el texto crudo del art√≠culo, se env√≠a al LLM (`gpt-5-nano`) para su an√°lisis.\n",
    "\n",
    "Se utiliza un *prompt* espec√≠fico que act√∫a como un asistente de an√°lisis de texto. Este *prompt* tiene reglas claras:\n",
    "\n",
    "```\n",
    "instructions = \"\"\"\n",
    "    You are a text analysis assistant specialized in identifying programming languages mentioned in academic or technical articles.\n",
    "    Analyze the provided raw text (extracted directly from a PDF). Identify and return the main programming languages mentioned in the article (do not include frameworks, libraries, or tools).\n",
    "    If a ‚ÄúReferences‚Äù or ‚ÄúBibliography‚Äù section appears, ignore all text after that marker.\n",
    "    Return strictly in JSON, like:\n",
    "    {\n",
    "        \"programming_languages\": [\"Python\", \"C\", \"Java\"]\n",
    "    }\n",
    "    If none found:\n",
    "    {\n",
    "        \"programming_languages\": []\n",
    "    }\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "#### ALMACENAMIENTO EN CACHE (CSV)\n",
    "El proceso completo est√° orquestado por la funci√≥n `process_all_obras`, que itera sobre cada art√≠culo del `obra.csv` y aplica los pasos anteriores. Los resultados se guardan para asegurar la persistencia de los datos:\n",
    "\n",
    "* **`tecnologia.csv`**: Este archivo act√∫a como una tabla maestra para las tecnolog√≠as. Cuando el LLM devuelve un lenguaje (ej: \"Python\"), el script comprueba (con `load_tecnologias`) si \"Python\" ya existe en este CSV. Si no existe, se le asigna un nuevo `id` (`next_tecn_id`) y se a√±ade al final.\n",
    "* **`obra_tecnologia.csv`**: Este archivo es la tabla de uni√≥n (relaci√≥n N-a-N). Por cada lenguaje identificado en un art√≠culo, se guarda una fila que conecta el `obra_id` con el `tecnologia_id` correspondiente (ej: `[id_enlace, 101, 1]` para vincular la obra 101 con Python (id 1)).\n",
    "\n",
    "Este sistema de cach√© es crucial, ya que permite reanudar el proceso en caso de error sin tener que volver a consumir recursos de la API del LLM sobre art√≠culos ya procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d558a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6010 obras in CSV.\n",
      "\n",
      "üîπ Processing Obra ID: 1\n",
      "‚ùå No valid PDF or text found.\n",
      "‚ùå No valid PDF or text found.\n",
      "‚ö†Ô∏è Skipping Obra ID 1, no text extracted.\n",
      "\n",
      "üîπ Processing Obra ID: 2\n",
      "‚ùå No valid PDF or text found.\n",
      "‚ùå No valid PDF or text found.\n",
      "‚ö†Ô∏è Skipping Obra ID 2, no text extracted.\n",
      "\n",
      "üîπ Processing Obra ID: 3\n",
      "‚ùå No valid PDF or text found.\n",
      "‚ùå No valid PDF or text found.\n",
      "‚ö†Ô∏è Skipping Obra ID 3, no text extracted.\n",
      "\n",
      "üîπ Processing Obra ID: 4\n",
      "üìÑ Text extracted for Obra ID 4 (1903 chars)\n",
      "üîó Source URL used: https://joss.theoj.org/papers/10.21105/joss.00024.pdf\n",
      "üìù Text preview: corner.py: Scatterplot matrices in Python Daniel Foreman-Mackey1 1 Sagan Fellow, University of Washington DOI: 10.21105/joss.00024 Software ‚Ä¢ Review ‚Ä¢ Repository ‚Ä¢ Archive Licence Authors of JOSS papers retain copyright and release the work un- der a Creative Commons Attri- bution 4.0 International...\n",
      "ü§ñ Analyzing text for Obra ID 4...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 375\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    370\u001b[39m     \u001b[38;5;66;03m#Uncomment if you are testing runs \u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"for csv_file in [TECN_CSV, OBRA_TECN_CSV]:\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m        if os.path.exists(csv_file):\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m            os.remove(csv_file)\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[33;03m            print(f\"üóëÔ∏è Deleted old CSV: {csv_file}\")\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[43mprocess_all_obras\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 345\u001b[39m, in \u001b[36mprocess_all_obras\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mü§ñ Analyzing text for Obra ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobra_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     result = \u001b[43manalyze_text_with_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    347\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è Analysis failed for Obra ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobra_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 284\u001b[39m, in \u001b[36manalyze_text_with_gpt\u001b[39m\u001b[34m(pdf_text, model)\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;66;03m# GPT-5 request\u001b[39;00m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m         response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m(\n\u001b[32m    285\u001b[39m             model=model,\n\u001b[32m    286\u001b[39m             \u001b[38;5;28minput\u001b[39m=\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[33m            You are a text analysis assistant specialized in identifying programming languages mentioned in academic or technical articles.\u001b[39m\n\u001b[32m    288\u001b[39m \n\u001b[32m    289\u001b[39m \u001b[33m            Task:\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[33m            1Ô∏è‚É£ Identify **only actual programming languages** used to write code.\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[33m            2Ô∏è‚É£ Do **NOT** include frameworks, libraries, standards, formal languages, or platforms.\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[33m            3Ô∏è‚É£ Ignore any text after a ‚ÄúReferences‚Äù or ‚ÄúBibliography‚Äù section.\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[33m            4Ô∏è‚É£ Return strictly in JSON:\u001b[39m\n\u001b[32m    294\u001b[39m \n\u001b[32m    295\u001b[39m \u001b[33m            \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mprogramming_languages\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPython\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mJava\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m]\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m    296\u001b[39m \n\u001b[32m    297\u001b[39m \u001b[33m            If none found, return:\u001b[39m\n\u001b[32m    298\u001b[39m \n\u001b[32m    299\u001b[39m \u001b[33m            \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mprogramming_languages\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: []\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \n\u001b[32m    302\u001b[39m \u001b[33m            Text:\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[33m            \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_text\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    305\u001b[39m         )\n\u001b[32m    306\u001b[39m         raw = response.output_text.strip()\n\u001b[32m    307\u001b[39m         json_start = raw.find(\u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\_utils\\_proxy.py:20\u001b[39m, in \u001b[36mLazyProxy.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     proxied = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_proxied__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxied, LazyProxy):\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m proxied  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\_utils\\_proxy.py:58\u001b[39m, in \u001b[36mLazyProxy.__get_proxied__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__get_proxied__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> T:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\_module_client.py:109\u001b[39m, in \u001b[36mResponsesProxy.__load__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__load__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Responses:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\functools.py:998\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    996\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    999\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1000\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\_client.py:265\u001b[39m, in \u001b[36mOpenAI.responses\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresponses\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Responses:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresources\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresponses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Responses\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Responses(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\resources\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbeta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     Beta,\n\u001b[32m      5\u001b[39m     AsyncBeta,\n\u001b[32m      6\u001b[39m     BetaWithRawResponse,\n\u001b[32m      7\u001b[39m     AsyncBetaWithRawResponse,\n\u001b[32m      8\u001b[39m     BetaWithStreamingResponse,\n\u001b[32m      9\u001b[39m     AsyncBetaWithStreamingResponse,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     Chat,\n\u001b[32m     13\u001b[39m     AsyncChat,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     AsyncChatWithStreamingResponse,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     Audio,\n\u001b[32m     21\u001b[39m     AsyncAudio,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     AsyncAudioWithStreamingResponse,\n\u001b[32m     26\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\resources\\beta\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbeta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     Beta,\n\u001b[32m      5\u001b[39m     AsyncBeta,\n\u001b[32m      6\u001b[39m     BetaWithRawResponse,\n\u001b[32m      7\u001b[39m     AsyncBetaWithRawResponse,\n\u001b[32m      8\u001b[39m     BetaWithStreamingResponse,\n\u001b[32m      9\u001b[39m     AsyncBetaWithStreamingResponse,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchatkit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     ChatKit,\n\u001b[32m     13\u001b[39m     AsyncChatKit,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     AsyncChatKitWithStreamingResponse,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mthreads\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     Threads,\n\u001b[32m     21\u001b[39m     AsyncThreads,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     AsyncThreadsWithStreamingResponse,\n\u001b[32m     26\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\resources\\beta\\beta.py:32\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mthreads\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mthreads\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     Threads,\n\u001b[32m     25\u001b[39m     AsyncThreads,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     AsyncThreadsWithStreamingResponse,\n\u001b[32m     30\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresources\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chat, AsyncChat\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrealtime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrealtime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     Realtime,\n\u001b[32m     34\u001b[39m     AsyncRealtime,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     37\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mBeta\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAsyncBeta\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBeta\u001b[39;00m(SyncAPIResource):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\resources\\beta\\realtime\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrealtime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     Realtime,\n\u001b[32m      5\u001b[39m     AsyncRealtime,\n\u001b[32m      6\u001b[39m     RealtimeWithRawResponse,\n\u001b[32m      7\u001b[39m     AsyncRealtimeWithRawResponse,\n\u001b[32m      8\u001b[39m     RealtimeWithStreamingResponse,\n\u001b[32m      9\u001b[39m     AsyncRealtimeWithStreamingResponse,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msessions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     Sessions,\n\u001b[32m     13\u001b[39m     AsyncSessions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     AsyncSessionsWithStreamingResponse,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtranscription_sessions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     TranscriptionSessions,\n\u001b[32m     21\u001b[39m     AsyncTranscriptionSessions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     AsyncTranscriptionSessionsWithStreamingResponse,\n\u001b[32m     26\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\resources\\beta\\realtime\\realtime.py:14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpx\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msessions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     Sessions,\n\u001b[32m     16\u001b[39m     AsyncSessions,\n\u001b[32m     17\u001b[39m     SessionsWithRawResponse,\n\u001b[32m     18\u001b[39m     AsyncSessionsWithRawResponse,\n\u001b[32m     19\u001b[39m     SessionsWithStreamingResponse,\n\u001b[32m     20\u001b[39m     AsyncSessionsWithStreamingResponse,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NOT_GIVEN, Query, Headers, NotGiven\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     is_azure_client,\n\u001b[32m     25\u001b[39m     maybe_transform,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     is_async_azure_client,\n\u001b[32m     29\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\resources\\beta\\realtime\\sessions.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_response\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_streamed_response_wrapper, async_to_streamed_response_wrapper\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_request_options\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbeta\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrealtime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m session_create_params\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbeta\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrealtime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msession_create_response\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SessionCreateResponse\n\u001b[32m     20\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mSessions\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAsyncSessions\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\types\\beta\\realtime\\__init__.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresponse_done_event\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResponseDoneEvent \u001b[38;5;28;01mas\u001b[39;00m ResponseDoneEvent\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msession_update_event\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SessionUpdateEvent \u001b[38;5;28;01mas\u001b[39;00m SessionUpdateEvent\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrealtime_client_event\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RealtimeClientEvent \u001b[38;5;28;01mas\u001b[39;00m RealtimeClientEvent\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrealtime_server_event\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RealtimeServerEvent \u001b[38;5;28;01mas\u001b[39;00m RealtimeServerEvent\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresponse_cancel_event\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResponseCancelEvent \u001b[38;5;28;01mas\u001b[39;00m ResponseCancelEvent\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kzzazzk\\Downloads\\gestbd-hackathon\\.venv\\Lib\\site-packages\\openai\\types\\beta\\realtime\\realtime_client_event.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresponse_cancel_event\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResponseCancelEvent\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresponse_create_event\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResponseCreateEvent\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtranscription_session_update\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TranscriptionSessionUpdate\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconversation_item_create_event\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationItemCreateEvent\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconversation_item_delete_event\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationItemDeleteEvent\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1322\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1262\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1532\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1506\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1605\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import json\n",
    "import openai\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz \n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_NAME = \"mistral:instruct\"\n",
    "PDF_TIMEOUT = 30\n",
    "UNPAYWALL_EMAIL = \"your_email@example.com\"\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = str(NOTEBOOK_DIR.parent)\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
    "\n",
    "OBRAS_CSV = os.path.join(CACHE_DIR, \"obra.csv\")\n",
    "TECN_CSV = os.path.join(CACHE_DIR, \"tecnologia.csv\")\n",
    "OBRA_TECN_CSV = os.path.join(CACHE_DIR, \"obra_tecnologia.csv\")\n",
    "\n",
    "\n",
    "instructions = \"\"\"You are a text analysis assistant specialized in identifying programming languages mentioned in academic or technical articles.\n",
    "Analyze the provided raw text (extracted directly from a PDF). Identify and return the main programming languages mentioned in the article (do not include frameworks, libraries, or tools).\n",
    "If a ‚ÄúReferences‚Äù or ‚ÄúBibliography‚Äù section appears, ignore all text after that marker.\n",
    "Return strictly in JSON, like:\n",
    "{\n",
    "  \"programming_languages\": [\"Python\", \"C\", \"Java\"]\n",
    "}\n",
    "If none found:\n",
    "{\n",
    "  \"programming_languages\": []\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------\n",
    "# CSV helpers\n",
    "# ----------------------\n",
    "def read_obras_from_csv(file_path):\n",
    "    obras = []\n",
    "    with open(file_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            obras.append((int(row[\"id\"]), row[\"direccion_fuente\"], row.get(\"doi\")))\n",
    "    return obras\n",
    "\n",
    "def init_csv(file_path, headers=None):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
    "        with open(file_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            if headers:\n",
    "                writer.writerow(headers)\n",
    "        return 1\n",
    "    max_id = 0\n",
    "    with open(file_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                val = row.get(\"id\")\n",
    "                if val:\n",
    "                    try:\n",
    "                        max_id = max(max_id, int(val))\n",
    "                    except:\n",
    "                        continue\n",
    "        except:\n",
    "            f.seek(0)\n",
    "            for line in f:\n",
    "                parts = line.split(\",\")\n",
    "                if parts:\n",
    "                    try:\n",
    "                        max_id = max(max_id, int(parts[0].strip()))\n",
    "                    except:\n",
    "                        continue\n",
    "    return max_id + 1\n",
    "\n",
    "def append_unique_to_csv(file_path, row, headers=None, key_index=1):\n",
    "    init_csv(file_path, headers=headers)\n",
    "    existing_keys = set()\n",
    "    with open(file_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        peek = next(reader, None)\n",
    "        if headers and peek and all(h in peek for h in headers):\n",
    "            pass\n",
    "        else:\n",
    "            if peek:\n",
    "                try:\n",
    "                    existing_keys.add(peek[key_index])\n",
    "                except:\n",
    "                    pass\n",
    "        for r in reader:\n",
    "            try:\n",
    "                existing_keys.add(r[key_index])\n",
    "            except:\n",
    "                continue\n",
    "    key = row[key_index] if len(row) > key_index else None\n",
    "    if key not in existing_keys:\n",
    "        with open(file_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "def append_to_csv(file_path, row, headers=None):\n",
    "    \"\"\"Simple append without uniqueness (for obra_tecnologia)\"\"\"\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    with open(file_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists and headers:\n",
    "            writer.writerow(headers)\n",
    "        writer.writerow(row)\n",
    "def load_tecnologias(file_path):\n",
    "    tech_map = {}\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                tech_map[row[\"nombre\"]] = int(row[\"id\"])\n",
    "    return tech_map\n",
    "\n",
    "# ----------------------\n",
    "# PDF + Analysis\n",
    "# ----------------------\n",
    "def get_text_from_pdf_url(pdf_url, doi=None):\n",
    "    tried_urls = set()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    def fetch_unpaywall_pdf(doi):\n",
    "        if not doi:\n",
    "            return None\n",
    "        try:\n",
    "            unpaywall_url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "            r = requests.get(unpaywall_url, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                pdf_link = data.get(\"best_oa_location\", {}).get(\"url_for_pdf\")\n",
    "                if pdf_link:\n",
    "                    print(f\"üìñ Found Unpaywall PDF: {pdf_link}\")\n",
    "                    return pdf_link\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Unpaywall fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_text_from_pdf_url(pdf_url, doi=None):\n",
    "    tried_urls = set()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    def fetch_unpaywall_pdf(doi):\n",
    "        if not doi:\n",
    "            return None\n",
    "        try:\n",
    "            unpaywall_url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "            r = requests.get(unpaywall_url, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                pdf_link = data.get(\"best_oa_location\", {}).get(\"url_for_pdf\")\n",
    "                if pdf_link:\n",
    "                    print(f\"üìñ Found Unpaywall PDF: {pdf_link}\")\n",
    "                    return pdf_link\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Unpaywall fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    def fetch_acm_pdf(doi):\n",
    "        if not doi:\n",
    "            return None\n",
    "        try:\n",
    "            acm_url = f\"https://dl.acm.org/doi/pdf/{doi}\"\n",
    "            r = requests.head(acm_url, allow_redirects=True, timeout=10)\n",
    "            if r.status_code == 200 and \"pdf\" in r.headers.get(\"Content-Type\", \"\").lower():\n",
    "                print(f\"üìÑ Found ACM PDF: {acm_url}\")\n",
    "                return acm_url\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è ACM fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    while pdf_url and pdf_url not in tried_urls:\n",
    "        tried_urls.add(pdf_url)\n",
    "        try:\n",
    "            response = requests.get(pdf_url, headers=headers, timeout=PDF_TIMEOUT)\n",
    "            content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
    "\n",
    "            if \"application/pdf\" in content_type:\n",
    "                try:\n",
    "                    pdf_bytes = io.BytesIO(response.content)\n",
    "                    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "                    text = \"\\n\\n\".join([page.get_text() for page in doc])\n",
    "                    if not text.strip():\n",
    "                        raise ValueError(\"No text extracted from PDF\")\n",
    "                    return text.strip(), pdf_url\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è PDF parse error with PyMuPDF: {e}\")\n",
    "                pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "                continue\n",
    "\n",
    "            if \"text/html\" in content_type:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                body_text = soup.get_text(separator=' ', strip=True).lower()\n",
    "                if any(x in body_text for x in [\"not found\", \"error 404\", \"no encontrado\", \"access denied\"]):\n",
    "                    pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "                    continue\n",
    "                text_labels = soup.select('[class*=\"textLayer\"], [id*=\"textLayer\"] div, span')\n",
    "                texts = [el.get_text(separator=' ', strip=True) for el in text_labels]\n",
    "                if texts:\n",
    "                    return \" \".join(texts), pdf_url\n",
    "                pdf_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.pdf')]\n",
    "                if pdf_links:\n",
    "                    next_pdf = requests.compat.urljoin(pdf_url, pdf_links[0])\n",
    "                    if next_pdf not in tried_urls:\n",
    "                        pdf_url = next_pdf\n",
    "                        continue\n",
    "                pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "                continue\n",
    "\n",
    "            pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Exception while fetching PDF: {e}\")\n",
    "            pdf_url = fetch_unpaywall_pdf(doi) or fetch_acm_pdf(doi)\n",
    "\n",
    "    print(\"‚ùå No valid PDF or text found.\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate the number of tokens in a string for GPT models.\"\"\"\n",
    "    return len(ENCODING.encode(text))\n",
    "\n",
    "def analyze_text(instructions, pdf_text):\n",
    "    detected_languages = set()\n",
    "\n",
    "    # 1Ô∏è‚É£ Try LLM first\n",
    "    if pdf_text.strip():\n",
    "        prompt = f\"{instructions}\\n\\nText:\\n{pdf_text}\"\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"run\", MODEL_NAME],\n",
    "                input=prompt,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                encoding=\"utf-8\",\n",
    "                errors=\"ignore\"\n",
    "            )\n",
    "            raw = result.stdout.strip()\n",
    "            json_start = raw.find(\"{\")\n",
    "            json_end = raw.rfind(\"}\") + 1\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                llm_result = json.loads(raw[json_start:json_end])\n",
    "                detected_languages.update(llm_result.get(\"programming_languages\", []))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return {\"programming_languages\": sorted(detected_languages)}\n",
    "\n",
    "# Make sure to set your API key in the environment\n",
    "# export OPENAI_API_KEY=\"sk-...\"\n",
    "def analyze_text_with_gpt(pdf_text, model=\"gpt-5-nano\"):\n",
    "    \"\"\"\n",
    "    Analyze PDF text using ChatGPT Responses API.\n",
    "    Returns a set of detected programming languages.\n",
    "    Automatically skips blocked content.\n",
    "    \"\"\"\n",
    "    detected_languages = set()\n",
    "\n",
    "    # Blocked content check\n",
    "    blocked_indicators = [\n",
    "        \"enable javascript and cookies to continue\",\n",
    "        \"access denied\",\n",
    "        \"not found\",\n",
    "        \"error 404\"\n",
    "    ]\n",
    "    preview_text = pdf_text[:300].replace(\"\\n\", \" \").lower()\n",
    "    if any(b in preview_text for b in blocked_indicators):\n",
    "        print(\"‚ö†Ô∏è Blocked content detected, skipping analysis.\")\n",
    "        return {\"programming_languages\": []}\n",
    "\n",
    "    # GPT-5 request\n",
    "    try:\n",
    "        response = openai.responses.create(\n",
    "            model=model,\n",
    "            input=f\"\"\"\n",
    "            You are a text analysis assistant specialized in identifying programming languages mentioned in academic or technical articles.\n",
    "\n",
    "            Task:\n",
    "            1Ô∏è‚É£ Identify **only actual programming languages** used to write code.\n",
    "            2Ô∏è‚É£ Do **NOT** include frameworks, libraries, standards, formal languages, or platforms.\n",
    "            3Ô∏è‚É£ Ignore any text after a ‚ÄúReferences‚Äù or ‚ÄúBibliography‚Äù section.\n",
    "            4Ô∏è‚É£ Return strictly in JSON:\n",
    "\n",
    "            {{\"programming_languages\": [\"Python\", \"C\", \"Java\"]}}\n",
    "\n",
    "            If none found, return:\n",
    "\n",
    "            {{\"programming_languages\": []}}\n",
    "\n",
    "\n",
    "            Text:\n",
    "            {pdf_text}\n",
    "\"\"\"\n",
    "        )\n",
    "        raw = response.output_text.strip()\n",
    "        json_start = raw.find(\"{\")\n",
    "        json_end = raw.rfind(\"}\") + 1\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            llm_result = json.loads(raw[json_start:json_end])\n",
    "            detected_languages.update(llm_result.get(\"programming_languages\", []))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è GPT analysis failed: {e}\")\n",
    "\n",
    "    return {\"programming_languages\": sorted(detected_languages)}\n",
    "\n",
    "# ----------------------\n",
    "# Main loop\n",
    "# ----------------------\n",
    "def process_all_obras():\n",
    "    obras = read_obras_from_csv(OBRAS_CSV)\n",
    "    print(f\"Found {len(obras)} obras in CSV.\")\n",
    "\n",
    "    next_tecn_id = init_csv(TECN_CSV, headers=[\"id\",\"nombre\"])\n",
    "    next_link_id = init_csv(OBRA_TECN_CSV, headers=[\"id\",\"obra_id\",\"tecnologia_id\"])\n",
    "\n",
    "    for obra_id, pdf_url, doi in obras:\n",
    "        print(f\"\\nüîπ Processing Obra ID: {obra_id}\")\n",
    "        try:\n",
    "            # Step 1: fetch PDF text\n",
    "            text, final_url = get_text_from_pdf_url(pdf_url, doi)\n",
    "            if not text:\n",
    "                print(f\"‚ùå No valid PDF or text found.\")\n",
    "                print(f\"‚ö†Ô∏è Skipping Obra ID {obra_id}, no text extracted.\")\n",
    "                continue\n",
    "            else:\n",
    "                preview = text[:300].replace(\"\\n\", \" \").strip()\n",
    "                print(f\"üìÑ Text extracted for Obra ID {obra_id} ({len(text)} chars)\")\n",
    "                print(f\"üîó Source URL used: {final_url}\")\n",
    "                print(f\"üìù Text preview: {preview}{'...' if len(text) > 300 else ''}\")\n",
    "\n",
    "            # Step 2: analyze with Ollama\n",
    "            print(f\"ü§ñ Analyzing text for Obra ID {obra_id}...\")\n",
    "            try:\n",
    "                result = analyze_text_with_gpt(text)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Analysis failed for Obra ID {obra_id}: {e}\")\n",
    "                result = {\"programming_languages\": []}\n",
    "\n",
    "            languages = result.get(\"programming_languages\", [])\n",
    "            print(f\"üìù Obra ID {obra_id} languages detected: {languages}\")\n",
    "\n",
    "            tech_map = load_tecnologias(TECN_CSV)  # { \"Python\": 89, \"C\": 90, ... }\n",
    "\n",
    "            for lang in languages:\n",
    "                if lang not in tech_map:\n",
    "                    tech_map[lang] = next_tecn_id\n",
    "                    append_to_csv(TECN_CSV, [next_tecn_id, lang], headers=[\"id\",\"nombre\"])\n",
    "                    next_tecn_id += 1\n",
    "\n",
    "                tecnologia_id = tech_map[lang]\n",
    "                append_to_csv(OBRA_TECN_CSV, [next_link_id, obra_id, tecnologia_id], headers=[\"id\",\"obra_id\",\"tecnologia_id\"])\n",
    "                next_link_id += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Unexpected error processing Obra ID {obra_id}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Uncomment if you are testing runs \n",
    "    \"\"\"for csv_file in [TECN_CSV, OBRA_TECN_CSV]:\n",
    "        if os.path.exists(csv_file):\n",
    "            os.remove(csv_file)\n",
    "            print(f\"üóëÔ∏è Deleted old CSV: {csv_file}\")\"\"\"\n",
    "    process_all_obras()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1701b",
   "metadata": {},
   "source": [
    "### ALMACENAMIENTO EN POSTGRESQL\n",
    "---\n",
    "\n",
    "Una vez declaradas las tablas en **PostgreSQL**, y los datos obtenidos mediante **OpenAlex**, y la API de **OpenAI**, y guardados en archivos *.csv (en la carpeta **cache**), procedemos a trasladar todos los datos guardados en dichos archivos y almacenarlos en nuestra base de datos.\n",
    "\n",
    "Primero nos conectamos a la base de datos y obtenemos un cursor, mediante la biblioteca `psycopg2-binary`.\n",
    "Segundo cargamos directamente todos los archivos de cache como `DataFrames` de `pandas`.\n",
    "\n",
    "`df_tematica = pd.read_csv(file_tematica)`\n",
    "\n",
    "Tabla a tabla, vamos leyendo cada fila de su `DataFrame` e insertandolo en su tabla correspondiente mediante **SQL**. Por ejemplo: el archivo tematica.csv sirve para rellenar la tabla tematica de la base de datos. En caso de conflito por datos repetidos, la orden de insercion se ignora.\n",
    "\n",
    "```\n",
    "for _, row in df_tematica.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tematica (id, nombre_campo)\n",
    "            VALUES (%s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", row['id'], row['nombre_campo']))\n",
    "```\n",
    "\n",
    "Una vez todos los datos de cache han sido cargados, guardamos los cambios desconectamos el cursor y cerramos la conexion.\n",
    "\n",
    "`connection.commit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a5a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV data loaded successfully including tecnologia and obra_tecnologia.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "DB_PARAMS = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"demoDB\",\n",
    "    \"user\": \"userPSQL\",\n",
    "    \"password\": \"passPSQL\"\n",
    "}\n",
    "\n",
    "def main():\n",
    "    connection = psycopg2.connect(**DB_PARAMS)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    notebook_dir = Path.cwd()\n",
    "    script_dir = str(notebook_dir.parent)\n",
    "    dir_cache = os.path.join(script_dir, \"cache\")\n",
    "\n",
    "\n",
    "    # File paths\n",
    "    file_tematica = os.path.join(dir_cache, 'tematica.csv')\n",
    "    file_tematica_contenida = os.path.join(dir_cache, 'tematica_contenida.csv')\n",
    "    file_obra = os.path.join(dir_cache, 'obra.csv')\n",
    "    file_tecnologia = os.path.join(dir_cache, 'tecnologia.csv')\n",
    "    file_obra_tecnologia = os.path.join(dir_cache, 'obra_tecnologia.csv')\n",
    "\n",
    "    # Read CSVs\n",
    "    df_tematica = pd.read_csv(file_tematica)\n",
    "    df_tematica_contenida = pd.read_csv(file_tematica_contenida)\n",
    "    df_obra = pd.read_csv(file_obra)\n",
    "    df_tecnologia = pd.read_csv(file_tecnologia)\n",
    "    df_obra_tecnologia = pd.read_csv(file_obra_tecnologia)\n",
    "\n",
    "    # Insert tematica\n",
    "    for _, row in df_tematica.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tematica (id, nombre_campo)\n",
    "            VALUES (%s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (int(row['id']), row['nombre_campo'].strip() if pd.notna(row['nombre_campo']) else None))\n",
    "\n",
    "    # Insert tematica_contenida\n",
    "    for _, row in df_tematica_contenida.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tematica_contenida (id, tematica_padre_id, tematica_hijo_id)\n",
    "            VALUES (%s, %s, %s)\n",
    "            ON CONFLICT DO NOTHING\n",
    "        \"\"\", (int(row['id']), int(row['id_padre']), int(row['id_hijo'])))\n",
    "\n",
    "    # Insert obra\n",
    "    for _, row in df_obra.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO obra (\n",
    "                id, doi, direccion_fuente, titulo, abstract, fecha_publicacion,\n",
    "                idioma, num_citas, fwci, tematica_id\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (\n",
    "            int(row['id']),\n",
    "            row.get('doi').strip() if pd.notna(row.get('doi')) else None,\n",
    "            row['direccion_fuente'].strip() if pd.notna(row.get('direccion_fuente')) else None,\n",
    "            row['titulo'].strip() if pd.notna(row.get('titulo')) else None,\n",
    "            row.get('abstract').strip() if pd.notna(row.get('abstract')) else None,\n",
    "            row.get('fecha_publicacion') if pd.notna(row.get('fecha_publicacion')) else None,\n",
    "            row.get('idioma').strip() if pd.notna(row.get('idioma')) else None,\n",
    "            int(row.get('num_citas', 0)) if pd.notna(row.get('num_citas')) else 0,\n",
    "            float(row.get('fwci', 0.0)) if pd.notna(row.get('fwci')) else 0.0,\n",
    "            int(row.get('tematica_id')) if pd.notna(row.get('tematica_id')) else None\n",
    "        ))\n",
    "\n",
    "    # Insert tecnologia\n",
    "    for _, row in df_tecnologia.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tecnologia (id, nombre, tipo, version)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (\n",
    "            int(row['id']),\n",
    "            row['nombre'].strip() if pd.notna(row['nombre']) else None,\n",
    "            row.get('tipo').strip() if pd.notna(row.get('tipo')) else None,\n",
    "            row.get('version').strip() if pd.notna(row.get('version')) else None\n",
    "        ))\n",
    "\n",
    "    # Insert obra_tecnologia\n",
    "    for _, row in df_obra_tecnologia.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO obra_tecnologia (id, obra_id, tecnologia_id)\n",
    "            VALUES (%s, %s, %s)\n",
    "            ON CONFLICT (id) DO NOTHING\n",
    "        \"\"\", (int(row['id']), int(row['obra_id']), int(row['tecnologia_id'])))\n",
    "# Puede que on conflict falle por lo de id \n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"‚úÖ CSV data loaded successfully including tecnologia and obra_tecnologia.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dadfffb",
   "metadata": {},
   "source": [
    "### Creaci√≥n del Grafo: Turtle, Schema.org & SKOS\n",
    "\n",
    "#### Mapeo de Entidades (PostgreSQL ‚Üí Vocabularios)\n",
    "\n",
    "En la √∫ltima entrega (Pr√°ctica 3) se defini√≥ un conjunto de mapeos entre las tablas de la base de datos _PostgreSQL_ y las clases de los vocabularios **SCHEMA** y **SKOS**.  \n",
    "Estos mapeos permiten representar las entidades relacionales (_obra_, _tem√°tica_, _tecnolog√≠a_, _obra_tecnolog√≠a_ y _tem√°tica_contenida_) como recursos sem√°nticos dentro de un grafo RDF.\n",
    "\n",
    "#### Generaci√≥n del Grafo RDF\n",
    "\n",
    "El siguiente paso consiste en **transformar los datos de la base de datos en un grafo RDF** y **exportarlos al formato Turtle (.ttl)**.  \n",
    "Este archivo podr√° ser cargado posteriormente en **GraphDB**, donde se podr√°n ejecutar consultas **SPARQL** sobre la informaci√≥n enlazada.\n",
    "\n",
    "El proceso se realiza mediante un script en **Python** que:\n",
    "\n",
    "1. Conecta con la base de datos PostgreSQL.  \n",
    "2. Itera sobre cada fila de las tablas relevantes.  \n",
    "3. Genera triples RDF utilizando las ontolog√≠as **SCHEMA** y **SKOS**.  \n",
    "4. Exporta el resultado final a un archivo `.ttl`.\n",
    "\n",
    "#### Descripci√≥n del Script\n",
    "\n",
    "El script realiza los siguientes pasos principales:\n",
    "\n",
    "- **Conexi√≥n a la base de datos** PostgreSQL mediante `psycopg2`.  \n",
    "- **Creaci√≥n de un grafo RDF** usando la librer√≠a `rdflib`.  \n",
    "- **Asignaci√≥n de namespaces**:  \n",
    "  - `schema:` ‚Üí [https://schema.org/](https://schema.org/)  \n",
    "  - `skos:` ‚Üí [https://www.w3.org/2004/02/skos/core#](https://www.w3.org/2004/02/skos/core#)  \n",
    "  - `openalex:` ‚Üí [https://openalex.org/](https://openalex.org/)  \n",
    "- **Iteraci√≥n sobre las tablas**:\n",
    "  - `tematica` ‚Üí Clases `skos:Concept`  \n",
    "  - `tematica_contenida` ‚Üí Relaciones jer√°rquicas `skos:broader` y `skos:narrower`  \n",
    "  - `tecnologia` ‚Üí Clases `schema:SoftwareApplication`  \n",
    "  - `obra` ‚Üí Clases `schema:TechArticle`  \n",
    "  - `obra_tecnologia` ‚Üí Relaciones `schema:mentions`  \n",
    "- **Exportaci√≥n del grafo** en formato Turtle (`.ttl`)  \n",
    "- **Cierre de la conexi√≥n** a la base de datos.\n",
    "\n",
    "#### Resultado Final\n",
    "\n",
    "El script genera un archivo RDF llamado **`openalex_graph.ttl`** dentro del directorio `ttl/`.  \n",
    "Este archivo contiene todos los triples RDF generados a partir de la base de datos y puede cargarse directamente en **GraphDB**.\n",
    "\n",
    "Una vez cargado, es posible ejecutar consultas **SPARQL** para:\n",
    "\n",
    "- Explorar relaciones entre obras y tecnolog√≠as.  \n",
    "- Visualizar jerarqu√≠as tem√°ticas mediante SKOS.  \n",
    "- Analizar la estructura sem√°ntica del conocimiento extra√≠do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "004db1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database ‚úÖ\n",
      "Mapped table: tematica ‚úÖ\n",
      "Mapped table: tecnologia ‚úÖ\n",
      "Mapped table: obra ‚úÖ\n",
      "Mapped relationships ‚úÖ\n",
      "RDF graph exported to ../ttl/openalex_graph.ttl üß©\n",
      "PostgreSQL connection closed üîí\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import psycopg2\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, SKOS, XSD\n",
    "\n",
    "# --- Namespaces ---\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "OPENALEX = Namespace(\"https://openalex.org/\")\n",
    "g = Graph()\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"skos\", SKOS)\n",
    "g.bind(\"openalex\", OPENALEX)\n",
    "\n",
    "# --- PostgreSQL connection ---\n",
    "DB_PARAMS = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"demoDB\",\n",
    "    \"user\": \"userPSQL\",\n",
    "    \"password\": \"passPSQL\"\n",
    "}\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(**DB_PARAMS)\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Connected to database ‚úÖ\")\n",
    "\n",
    "# --- 1. TEM√ÅTICA ---\n",
    "cur.execute(\"SELECT id, nombre_campo FROM tematica;\")\n",
    "for tmid, nombre in cur.fetchall():\n",
    "    tema_uri = OPENALEX[f\"tematica_{tmid}\"]\n",
    "    g.add((tema_uri, RDF.type, SKOS.Concept))\n",
    "    g.add((tema_uri, SKOS.prefLabel, Literal(nombre)))\n",
    "\n",
    "print(\"Mapped table: tematica ‚úÖ\")\n",
    "\n",
    "# tematica_contenida ‚Üí skos:broader\n",
    "cur.execute(\"SELECT id, tematica_padre_id, tematica_hijo_id FROM tematica_contenida;\")\n",
    "for tcid, parent, child in cur.fetchall():\n",
    "    parent_uri = OPENALEX[f\"tematica_{parent}\"]\n",
    "    child_uri = OPENALEX[f\"tematica_{child}\"]\n",
    "    g.add((parent_uri, SKOS.narrower, child_uri))\n",
    "    g.add((child_uri, SKOS.broader, parent_uri))\n",
    "\n",
    "# --- 2. TECNOLOG√çA ---\n",
    "cur.execute(\"SELECT id, nombre, tipo, version FROM tecnologia;\")\n",
    "for tid, nombre, tipo, version in cur.fetchall():\n",
    "    tech_uri = OPENALEX[f\"tecnologia_{tid}\"]\n",
    "    g.add((tech_uri, RDF.type, SCHEMA.SoftwareApplication))\n",
    "    g.add((tech_uri, SCHEMA.name, Literal(nombre)))\n",
    "    if tipo:\n",
    "        g.add((tech_uri, SCHEMA.applicationCategory, Literal(tipo)))\n",
    "    if version:\n",
    "        g.add((tech_uri, SCHEMA.softwareVersion, Literal(version)))\n",
    "\n",
    "print(\"Mapped table: tecnologia ‚úÖ\")\n",
    "\n",
    "# --- 3. OBRA ---\n",
    "cur.execute(\"SELECT id, doi, direccion_fuente, titulo, abstract, fecha_publicacion, idioma, num_citas, fwci, tematica_id FROM obra;\")\n",
    "for oid, doi, direccion_fuente, titulo, abstract, fecha_publicacion, idioma, num_citas, fwci, tematica_id in cur.fetchall():\n",
    "    obra_uri = OPENALEX[f\"obra_{oid}\"]\n",
    "    g.add((obra_uri, RDF.type, SCHEMA.TechArticle))\n",
    "    if doi:\n",
    "        g.add((obra_uri, SCHEMA.sameAs, Literal(doi)))\n",
    "    if direccion_fuente:\n",
    "        g.add((obra_uri, SCHEMA.url, Literal(direccion_fuente)))\n",
    "    if titulo:\n",
    "        g.add((obra_uri, SCHEMA.name, Literal(titulo)))\n",
    "    if abstract:\n",
    "        g.add((obra_uri, SCHEMA.abstract, Literal(abstract)))\n",
    "    if fecha_publicacion:\n",
    "        g.add((obra_uri, SCHEMA.datePublished, Literal(fecha_publicacion, datatype=XSD.date)))\n",
    "    if idioma:\n",
    "        g.add((obra_uri, SCHEMA.inLanguage, Literal(idioma)))\n",
    "    if num_citas:\n",
    "        g.add((obra_uri, SCHEMA.citationCount, Literal(num_citas, datatype=XSD.integer)))\n",
    "    if fwci:\n",
    "        g.add((obra_uri, SCHEMA.metric, Literal(fwci, datatype=XSD.float)))\n",
    "    if tematica_id:\n",
    "        g.add((obra_uri, SCHEMA.about, OPENALEX[f\"tematica_{tematica_id}\"]))\n",
    "\n",
    "print(\"Mapped table: obra ‚úÖ\")\n",
    "\n",
    "# obra_tecnologia ‚Üí schema:mentions\n",
    "cur.execute(\"SELECT obra_id, tecnologia_id FROM obra_tecnologia;\")\n",
    "for oid, tid in cur.fetchall():\n",
    "    g.add((OPENALEX[f\"obra_{oid}\"], SCHEMA.mentions, OPENALEX[f\"tecnologia_{tid}\"]))\n",
    "\n",
    "\n",
    "print(\"Mapped relationships ‚úÖ\")\n",
    "\n",
    "# --- 4. EXPORT ---\n",
    "output_file = \"../ttl/openalex_graph.ttl\"\n",
    "g.serialize(destination=output_file, format=\"turtle\")\n",
    "print(f\"RDF graph exported to {output_file} üß©\")\n",
    "\n",
    "# --- 5. Cleanup ---\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"PostgreSQL connection closed üîí\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ebbb30",
   "metadata": {},
   "source": [
    "### CREACI√ìN DE CONSULTAS\n",
    "---\n",
    "\n",
    "Para finalizar el proyecto, hemos creado una serie de querys con las que se pueda analizar s√≠ el sistema cumple los objetivos programados.\n",
    "\n",
    "Primeramente, se prob√≥ si cumpl√≠a el objetivo principal: obtener el n√∫mero de apariciones de distintos lenguajes de programaci√≥n en obras de cada subtem√°tica (o tem√°tica sin hijas) distinta. El resultado fue una tabla donde se listaban todas las combinaciones de t√≥pico y lenguaje distinto dentro de la base de datos, junto al n√∫mero de veces que se repet√≠a esa relaci√≥n. Por lo tanto, se cumpli√≥ la meta descrita al inicio del proyecto.\n",
    "\n",
    "Para demostrar su funcionamiento, solo hay que utilizar la siguiente consulta en GraphDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PREFIX schema: <https://schema.org/>\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "\n",
    "# Queremos una lista de \"aristas\" (conexiones)\n",
    "SELECT ?topicName ?techName (COUNT(DISTINCT ?work) AS ?sharedWorksCount)\n",
    "WHERE {\n",
    "    ?work schema:about ?topic .\n",
    "    ?work schema:mentions ?technology .\n",
    "    \n",
    "    OPTIONAL { ?topic skos:prefLabel ?topicName . }\n",
    "    OPTIONAL { ?technology schema:name ?techName . }\n",
    "    \n",
    "    FILTER(BOUND(?topicName) && BOUND(?techName))\n",
    "}\n",
    "GROUP BY ?topicName ?techName\n",
    "ORDER BY DESC(?sharedWorksCount)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8cb0d8",
   "metadata": {},
   "source": [
    "Posteriormente, para exprimir los l√≠mites del sistema, decidimos probar una consulta m√°s: Tecnolog√≠as m√°s competitivas entre campos. En esta consulta, se lista el n√∫mero de veces que cada par de tecnolog√≠as distintas (por ejemplo, Python y Java, C++ y Prolog, etc.) aparecen en las mismas obras. Con el resultado, aprendimos que Python y Java eran la dupla que m√°s aparec√≠a en las mismas obras.\n",
    "\n",
    "Para demostrar su funcionamiento, solo hay que utilizar la siguiente consulta en GraphDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PREFIX schema: <https://schema.org/>\n",
    "\n",
    "SELECT ?techA_name ?techB_name (COUNT(DISTINCT ?topic) AS ?commonTopics)\n",
    "WHERE {\n",
    "    # Encuentra la primera tecnolog√≠a (A) mencionada por una obra\n",
    "    ?workA schema:mentions ?techA .\n",
    "    ?workA schema:about ?topic .\n",
    "    ?techA schema:name ?techA_name .\n",
    "\n",
    "    # Encuentra la segunda tecnolog√≠a (B) mencionada por la misma obra (o una obra sobre el mismo tema)\n",
    "    ?workB schema:mentions ?techB .\n",
    "    ?workB schema:about ?topic . # <-- Mismo tema\n",
    "    ?techB schema:name ?techB_name .\n",
    "\n",
    "    # Asegura que no sea la misma tecnolog√≠a\n",
    "    FILTER (?techA != ?techB)\n",
    "\n",
    "    # Solo queremos los nombres, no las URIs largas\n",
    "    FILTER(BOUND(?techA_name) && BOUND(?techB_name))\n",
    "}\n",
    "GROUP BY ?techA_name ?techB_name\n",
    "ORDER BY DESC(?commonTopics)\n",
    "LIMIT 10\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a167b4ee",
   "metadata": {},
   "source": [
    "### FUTUROS INCREMENTOS\n",
    "---\n",
    "Actualmente, en la tabla de tecnolog√≠as almacenamos cerca de 600 filas. Al ver la tabla m√°s de cerca, en realidad, muchas de las tecnolog√≠as utilizadas no son mas que frameworks o derivaciones de ya existentes, por ejemplo, si se busca el lenguaje **Java** en la lista, aparece cerca de 10 veces (sin contar **javascript**): Java, Java-K, Featherweight Java, etc.; y solo Java es un lenguaje de programacion, los demas son derivaciones que usan Java como base. Por degracia, nuestro LLM encargado de encontrar cada tecnologia usada no otorga el lenguaje principal en el que se basa el obtenido, asi que hemos terminado una lista de 600 tecnolog√≠as, que seria interesante simplificar para ajustarnos a nuestros objetivos.\n",
    "\n",
    "Proponemos como soluci√≥n, reducir tabla de relaciones, eleiminando y sustiuyendo aquellas tecnolog√≠as que no sean lenguajes de programaci√≥n, por aquellas de las que deriva, por ejemplo, sustituir Java-K por Java; y propagar los cambios a la tabla obras y actualizar las relaciones antiguas por las nuevas.\n",
    "\n",
    "Para ello, identificaremos primero cuales tecnolog√≠as son lenguajes de programaci√≥n y cuales no, utilizando la base de datos con API **Wikidata**, que tiene clase titulada `programming language`. Una vez obtenido cuales son lenguajes de programaci√≥n, proponemos varios m√©todos, para determinar la herencia entre tecnolog√≠as.\n",
    "\n",
    "El primer metodo consisten en utilizar embeddings para transformar las tecnolog√≠as, y aplicar el algoritmo K-Means para detectar los grupos que se forman entorno a cada lenguaje. Sin embargo, este algoritmo puede presentar varios problemas, siendo uno de ellos, que m√∫ltiples lenguajes de programaci√≥n acaben en un mismo cluster, y otras tecnolog√≠as aparezcan aisladas en un grupo aparte.\n",
    "\n",
    "Otra soluci√≥n que proponemos, es utilizar de nuevo embeddings, pero en esta vez utilizar el algoritmo K-Nearest Neighbors (K-NN), que teniendo en cuenta cuales son los lenguajes de programaci√≥n, organize cada tecnolog√≠a no identificada como lenguaje alrededor de una que si lo es."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce5acd",
   "metadata": {},
   "source": [
    "### PARTICIPANTES:\n",
    "* Jaime Vaquero Rabahieh. Correo: jaime.vaquero@alumnos.upm.es\n",
    "* Zakaria Lasry Sahraoui. Correo: z.lsahraoui@alumnos.upm.es\n",
    "* Damian Sanchez Maqueda. Correo: damian.sanchez@alumnos.upm.es\n",
    "* Radu-Andrei Bourceanu. Correo: r.bourceanu@alumnos.upm.es"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
